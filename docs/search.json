[
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Digital Competence in the Biological Sciences",
    "section": "Course overview",
    "text": "Course overview\nThis course introduces biology students to digital competence in data handling using Excel and R. Excel and R are used as complementary tools but principels are software agnostic within a broader best practices in data lifecycle, from data entry to reproducible analysis. Thus, the focus is not on programming for its own sake, but on understanding how biological data are: - recorded, - structured, - summarized, - transformed, - interpreted, - and communicated.\nThe course is hands-on, biology-driven, and designed for students with no prior programming experience.\n\nThe lecture introduces concepts (data, statistics, algorithms, modelling).\nThis course makes those ideas tangible through practice.\n\nTopics such as inferential statistics, formal modelling, and machine learning are not taught here, but students are conceptually prepared for them.",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "index.html#teaching-philosophy",
    "href": "index.html#teaching-philosophy",
    "title": "Digital Competence in the Biological Sciences",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\n\n\n\n\n\n\nNote\n\n\n\nThis is not a YouTube tutorial. Interrupt me. Ask questions. Tell me when something doesn‚Äôt make sense. Dialogue makes this course better for everyone.\n\n\n\nüß† Critical thinking and context trump technical skills We‚Äôre not here to memorize functions or paths in Excel. We‚Äôre here to think carefully about what data mean, how they‚Äôre collected, and why they matter.\nüéØ Tools serve questions, not the other way around R and Excel are just tools. You should always know what you‚Äôre trying to ask or answer first.\nüß© Structure matters more than software Whether it‚Äôs a notebook or a database, how your data are organized determines what‚Äôs possible downstream.\nüîÅ Reproducibility is part of scientific responsibility Your future self (and your collaborators) need to understand what you did and why. Think of code and data as shared lab notes.\nüêõ Errors are normal and informative They‚Äôre not signs of failure‚Äîthey‚Äôre clues. Every bug is a question you‚Äôve been invited to ask.\n‚öñÔ∏è Understanding limits is as important as getting results No tool or model gives truth. Knowing when something breaks matters more than making it work.",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "index.html#case-study",
    "href": "index.html#case-study",
    "title": "Digital Competence in the Biological Sciences",
    "section": "Case study",
    "text": "Case study\nAll exercises are based on a real-world biodiversity monitoring and conservation scenarios from a national protected area in Belize. Data are inspired by actual monitoring programs, but simplified for teaching purposes and modified to avoid unnecessary sensitive information sharing.\nUsing a single evolving case study will allow you to focus on: - reasoning instead of context switching, - continuity across days, - real world data problems rather than artificial examples.",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Digital Competence in the Biological Sciences",
    "section": "Course structure",
    "text": "Course structure\nThe course runs for 5 days (3 hours each). Each day builds on the previous one and is divided into three 45 minutes lectures and/or exercises with two 15 minute breaks in between and a final 15 minute wrap-up sessions.\n\n\nDay 1 ‚Äì What ‚Äògood‚Äô Data Is \nFrom observations to structured tables (Excel in context)\n\nObservations, variables, metadata\nData as a simplified representation of reality\nRisks of inconsistency and missing context\nThe Good, the Bad and the Ugly in Excel\nData life cycle and FAIR principles overview\nDesigning analysis-ready tables and entry templates\n\nKey idea\nAll datasets are already models of reality. Basic organization of data. Basic data quality issues.\n\n\n\nDay 2 ‚Äì Describing Data \nDescriptive statistics and visualization (Excel to R)\n\nSummary statistics: centre, spread, outliers\nMean, median, variance\nDistributions and variability\nVisualizations as summaries, not proofs\nHonest and misleading plots\nRepeated tasks and scalability\nTransition from Excel to R\nFirst R contact: import data, compute summaries, make a plot\nReading and writing: CSV/TSV safely, encoding basics\n\n\n\n\nDay 3 ‚Äì From repetition to algorithms\nWhy R exists\n\nAlgorithms as explicit procedures\nProgramming logic 101 and pseudocode\nCore Atomic Data Types in R including missing values (e.g.¬†NA, NaN, Inf, -Inf, NULL, etc.)\nCommon Data Structures (vectors, lists, data frames, matrices, arrays)\nVectorized thinking Loops and conditionals\nFunctions and arguments\nFunctions: using them, then writing tiny ones\nPackages and R philosophy\n\n\n\n\nDay 4 ‚Äì Cleaning data and making work reproducible\nR as a data workbench\n\nCleaning as scientific work: decisions, assumptions, traceability\nTidy data: reshaping and joining tables\nQuality control checks as code, CI, and tests\nDocumentation: comments, READMEs, and metadata\nReproducibility: scripts, projects, folder structure\nData cleaning as scientific work\nData transformation and reshaping\nScripts as documentation\nReproducible workflows\nVersion control basics (Git)\nCreating automated reports with R (Quarto: one-button from raw data to figures pdfs, docs and websites)\n\n\n\n\nDay 5 ‚Äì Tools, models, and scientific responsibility\nSynthesis and outlook\n\nModels as simplified representations\nOfficial course evaluation (admin part)\nTool choice as a scientific decision\nWhat you wanted (from previous daily feed backs or unfinished business)\nRemembering strengths and limits of Excel and R in the Data Lifecycle\nCourse reflection and open discussion",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "index.html#what-you-will-be-able-to-do-after-this-course",
    "href": "index.html#what-you-will-be-able-to-do-after-this-course",
    "title": "Digital Competence in the Biological Sciences",
    "section": "What you will be able to do after this course",
    "text": "What you will be able to do after this course\nBy the end of the course, you will be able to:\n\nGain an overview of what excel and scripting languages can and can not do\nOrganize biological data in a structured and transparent way\nUse Excel appropriately for data entry and exploration\nUse R for basic data cleaning, summaries, and plots\nUnderstand what algorithms and models are at a conceptual level\nJudge which tool is appropriate for a given task\nDocument workflows clearly for reuse and reproducibility",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-website",
    "href": "index.html#how-to-use-this-website",
    "title": "Digital Competence in the Biological Sciences",
    "section": "How to use this website",
    "text": "How to use this website\n\nThis website is the main source for the course\nAll materials, exercises, and datasets are available here\nSlides are optional; the pages themselves are complete\nIf something is not documented here, it is not required or a mistake, please let me know!",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "index.html#looking-ahead",
    "href": "index.html#looking-ahead",
    "title": "Digital Competence in the Biological Sciences",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe skills and concepts introduced here form the foundation for: - general data management, - statistical inference, - modelling, - bioinformatics, - data-driven biological research. - and real world biological work.\n\n\n\n\n\n\nNote\n\n\n\nThis course is not the end of digital competence training.\nIt is the starting point.",
    "crumbs": [
      "Digital Competence in the Biological Sciences"
    ]
  },
  {
    "objectID": "days/day04.html",
    "href": "days/day04.html",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\nappreciate the importance of data cleaning as a scientific task\napply the principles of tidy data to reorganize datasets for analysis\nperform basic data transformations in R (filtering, recoding, reshaping, merging datasets)\nimplement data quality control checks using code (e.g.¬†finding outliers, inconsistencies)\ndocument your data processing steps (through comments, scripts, and README files)\nunderstand how to structure an analysis project (folders, scripts) for reproducibility\nexplain the benefits of version control (Git) and use basic Git commands or workflows to track changes\ncreate a simple reproducible report using R and Quarto or R Markdown",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day04.html#learning-goals",
    "href": "days/day04.html#learning-goals",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\nappreciate the importance of data cleaning as a scientific task\napply the principles of tidy data to reorganize datasets for analysis\nperform basic data transformations in R (filtering, recoding, reshaping, merging datasets)\nimplement data quality control checks using code (e.g.¬†finding outliers, inconsistencies)\ndocument your data processing steps (through comments, scripts, and README files)\nunderstand how to structure an analysis project (folders, scripts) for reproducibility\nexplain the benefits of version control (Git) and use basic Git commands or workflows to track changes\ncreate a simple reproducible report using R and Quarto or R Markdown",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day04.html#motivation",
    "href": "days/day04.html#motivation",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "Motivation",
    "text": "Motivation\n‚ÄúData cleaning‚Äù might sound mundane, but in practice it‚Äôs one of the most critical and time-consuming parts of any analysis ‚Äì and it‚Äôs deeply scientific. Decisions made during cleaning (like how to handle missing values or outliers) can affect results. Thus, cleaning must be done carefully and transparently, so others (and your future self) can understand and trust the final data. Today, we shift our mindset from ad-hoc fixes to systematic, documented transformations. We‚Äôll learn how to make our workflow reproducible ‚Äì meaning anyone can follow the same steps and get the same results. This involves using scripts, organizing our work, and leveraging tools like Git for version control and Quarto for integrated reporting.\nIn short, we want to move from a one-time analysis (that might be messy and irreproducible) to a robust process that can be redone or updated with new data at the click of a button.",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day04.html#agenda",
    "href": "days/day04.html#agenda",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "Agenda",
    "text": "Agenda\n13:30‚Äì14:15 | Block 1 ‚Äì Data cleaning as scientific work\n- Why cleaning is not ‚Äújanitorial‚Äù ‚Äì it‚Äôs part of science (documenting assumptions and decisions)\n- Common data problems: typos, missing values, out-of-range values, inconsistent coding (e.g.¬†‚ÄúM‚Äù vs ‚ÄúMale‚Äù)\n- Introduction to tidy data principles (each variable = one column, each observation = one row, etc.)\n- Reshaping data: wide vs long format (when and how to pivot tables)\n- Combining data: merging/joining tables from different sources (keys, primary identifiers)\n- Hands-on examples of tidying a small messy dataset (e.g.¬†splitting one column into two, unpivoting a summary table)\n14:15‚Äì14:30 | Break ‚òïÔ∏è\n14:30‚Äì15:15 | Block 2 ‚Äì Quality control and documentation\n- Writing R code to find potential errors (e.g., values outside expected range, duplicates where there shouldn‚Äôt be, mismatched categories)\n- Implementing simple tests/assertions in code (for example, verify that sample IDs are unique, or that dates are in chronological order)\n- Using scripts as a log of transformations: every change to data is recorded in code, rather than lost in manual edits\n- Commenting your code and organizing it into sections (making it readable and explaining the ‚Äúwhy‚Äù behind changes)\n- Creating a README file or data dictionary to capture metadata and decisions (what was done, when, by whom, and why)\n- Collaborative aspect: how clean, documented data and code allow others to build on your work\n15:15‚Äì15:30 | Break ‚òïÔ∏è\n15:30‚Äì16:15 | Block 3 ‚Äì Reproducible workflows and tools\n- Project structure: setting up an RStudio Project (or dedicated working directory) with subfolders (data/, scripts/, output/)\n- Using Git for version control: basic concepts (repository, commit, push/pull, conflict) and why ‚Äúsave early, save often‚Äù with history is beneficial\n- Demonstration of tracking a script or dataset with Git (seeing changes over time, reverting if needed)\n- Introduction to Quarto/R Markdown: integrating code and narrative to generate reports (HTML, PDF, etc.)\n- Rendering a reproducible report: one command to import raw data, run all analysis code, and output results (figures, tables) in a formatted report\n- Showcase: how this course material is itself a reproducible document created with Quarto\n- Tips for reproducibility: set seeds for randomness, record package versions for computational environments\n16:15‚Äì16:30 | Reflection, discussion, and outlook üß†\n- What aspects of cleaning or reproducibility will you apply in your own projects first?\n- Any remaining pain points in your data workflow that we haven‚Äôt addressed?\n- How do you plan to continue improving your digital/data skills after the course?\n- Preview of Day 5: wrapping up with models, tool choices, and looking ahead",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day04.html#cleaning-data-from-messy-to-tidy",
    "href": "days/day04.html#cleaning-data-from-messy-to-tidy",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "Cleaning data: from messy to tidy",
    "text": "Cleaning data: from messy to tidy\nReal-world data is messy. In the Belize case study, imagine we receive species observation logs from multiple rangers ‚Äì one uses \"Panthera onca\" for jaguar, another just writes \"Jaguar\", some cells are left blank, and one file uses -999 to denote a missing value. If we naively combine these, we‚Äôll have inconsistencies and errors that could lead to incorrect conclusions.\nData cleaning involves: - Detecting errors or inconsistencies (e.g., impossible values, typos, different units mixed in). - Deciding what to do with them (correct, exclude, flag as uncertain). - Implementing those corrections systematically.\nIt‚Äôs not just about making the dataset ‚Äúpretty‚Äù ‚Äì it‚Äôs about ensuring the data accurately reflect reality (as much as possible) and are structured to facilitate analysis.\n\nTidy data principles\nA cornerstone of modern data handling is the concept of tidy data (Wickham 2014). It provides a consistent structure that many analysis tools in R assume: 1. Each variable forms a column. 2. Each observation (case) forms a row. 3. Each value is a single cell.\nBoth wide and tidy (long) formats represent the same information in different layouts. Wide format data is often human-friendly: each observational unit (e.g.¬†a site or species) appears once as a row, and related variables (such as different time points or categories) are spread across multiple columns. Long-form structure is often preferred for analysis and visualization in tools like R or Python, even if the wide form is convenient for data entry or presentation. Below are two real-world inspired examples (with simplified data) from ecology that illustrate wide vs tidy data formats following best practices.\n\nExample 1: Amphibian Species Counts Over Years\nScenario: Researchers monitored the population of different frog species in a wetland from 2018 to 2021, conducting annual counts of each species.\nWide Format\n\n\n\nSpecies\n2018\n2019\n2020\n2021\n\n\n\n\nAmerican Bullfrog\n5\n7\n6\n8\n\n\nNorthern Leopard Frog\n12\n15\n18\n20\n\n\nGray Treefrog\n30\n25\n28\n32\n\n\n\nTidy Format\n\n\n\nSpecies\nYear\nCount\n\n\n\n\nAmerican Bullfrog\n2018\n5\n\n\nAmerican Bullfrog\n2019\n7\n\n\nAmerican Bullfrog\n2020\n6\n\n\nAmerican Bullfrog\n2021\n8\n\n\nNorthern Leopard Frog\n2018\n12\n\n\nNorthern Leopard Frog\n2019\n15\n\n\nNorthern Leopard Frog\n2020\n18\n\n\nNorthern Leopard Frog\n2021\n20\n\n\nGray Treefrog\n2018\n30\n\n\nGray Treefrog\n2019\n25\n\n\nGray Treefrog\n2020\n28\n\n\nGray Treefrog\n2021\n32\n\n\n\n\n\nExample 2: Habitat Cover by Type in Different Zones\nScenario: A wildlife reserve is divided into several management zones (North, South, East). Within each zone, land cover was surveyed to measure the area of different habitat types (Forest, Grassland, Wetland).\nWide Format\n\n\n\nZone\nForest (ha)\nGrassland (ha)\nWetland (ha)\n\n\n\n\nNorth\n120\n50\n30\n\n\nSouth\n80\n110\n20\n\n\nEast\n60\n40\n90\n\n\n\nTidy Format\n\n\n\nZone\nHabitat Type\nArea (ha)\n\n\n\n\nNorth\nForest\n120\n\n\nNorth\nGrassland\n50\n\n\nNorth\nWetland\n30\n\n\nSouth\nForest\n80\n\n\nSouth\nGrassland\n110\n\n\nSouth\nWetland\n20\n\n\nEast\nForest\n60\n\n\nEast\nGrassland\n40\n\n\nEast\nWetland\n90\n\n\n\n\nBoth examples illustrate that while wide format is useful for data collection and human readability, tidy format is essential for effective filtering, summarizing, and plotting in analysis workflows.\nTidy data is a prerequisite for most modern data science tools, particularly in R (e.g., with ggplot2, dplyr, or tidyr). If you need a dataset in tidy format you can reshape it using tools (in R, packages like tidyr provide functions like pivot_longer and pivot_wider to convert between long (tidy) and wide forms).\n\n\nExample of reshaping\nSuppose you have a spreadsheet where each column is a separate month‚Äôs measurements:\n\n\n\nsite\nJan_count\nFeb_count\nMar_count\n\n\n\n\n1\n5\n8\n6\n\n\n2\n3\n4\n7\n\n\n\nThis is wide. To tidy it, we‚Äôd transform it to:\n\n\n\nsite\nmonth\ncount\n\n\n\n\n1\nJan\n5\n\n\n1\nFeb\n8\n\n\n1\nMar\n6\n\n\n2\nJan\n3\n\n\n2\nFeb\n4\n\n\n2\nMar\n7\n\n\n\nNow ‚Äúmonth‚Äù is a variable. This format might have more rows, but it‚Äôs consistent and easier to feed into R‚Äôs plotting or modeling functions (e.g., you can easily plot count vs month, grouped by site).\nThis is how to do it in R\n\n# Load packages\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Create the wide data frame\ndf_wide &lt;- data.frame(\n  site = c(1, 2),\n  Jan_count = c(5, 3),\n  Feb_count = c(8, 4),\n  Mar_count = c(6, 7)\n)\n\n# Convert to tidy (long) format\ndf_tidy &lt;- df_wide %&gt;%             # Start with the wide data frame and pipe it forward\n  pivot_longer(\n    cols = ends_with(\"_count\"),   # Select columns whose names end in \"_count\" (i.e. Jan_count, Feb_count, etc.)\n    names_to = \"month\",           # Extract the column names into a new column called \"month\"\n    values_to = \"count\",          # Extract the cell values into a new column called \"count\"\n    names_prefix = \"\",            # No common prefix to remove (could be used if all columns had e.g. \"count_\" as prefix)\n    names_transform = list(month = ~ sub(\"_count\", \"\", .)  # Clean the \"month\" values by removing \"_count\" from the original column names\n    )\n  )\n\n# View result\nprint(df_tidy)\n\nConversely, sometimes data is too long and needs to be widened (less common in raw data, more in presenting results).\n\n\nMerging datasets\nOften, information is split across multiple tables that need to be joined. For example, you might have: - Table A: observations (with columns like species_code, count, location). - Table B: species metadata (with columns like species_code, latin_name, conservation_status).\nTo analyze or report, you may want to merge Table B into Table A so each observation row has the full species name and status. This requires a key (here species_code) that is present in both tables. When merging (or joining as database folks say), it‚Äôs important to ensure: - The key is consistent (no typos, same case, etc. ‚Äì cleaning might be needed here). - The join type is appropriate (usually an inner join: keep only rows that match in both tables; or a left join: keep all observations and add info from metadata, leaving NA if no match).\nIn R, base function merge() or dplyr::left_join(), dplyr::inner_join() can do this. Example:\nmerged_data &lt;- merge(observations, species_info, by = \"species_code\")\nThis adds columns from species_info to observations where species_code matches. A common pitfall is when keys don‚Äôt match due to inconsistencies. For instance, if species_code in one table has ‚ÄúPantheraOnca‚Äù and in the other ‚Äúpanthera_onca‚Äù or a blank for unknown species ‚Äì those won‚Äôt merge. Identifying and fixing these is part of cleaning: - Use consistent coding schemes (maybe standardize to all lowercase with underscores). - Fill in missing identifiers if possible, or decide to exclude those records.\n\n\n\n\n\n\nGroup exercise: Spot the mess (10 min)\n\n\n\nConsider the following snippet of a dataset (imagine it‚Äôs loaded as a data frame field_data in R): plot_id elevation_m species_1_count species_2_count notes A01 120 5 2 one tree measured A02 95m 3 NA\nA03 110 0 1 seedlings included? A-04 . 7 3\nIdentify at least 5 issues or inconsistencies in this data snippet that you would need to address during cleaning. (Assume species_1 and species_2 refer to counts of two different species in the plot.)\n\n\nPossible issues\n\n‚Ä¢ Inconsistent plot_id format: ‚ÄúA01‚Äù, ‚ÄúA02‚Äù, ‚ÄúA03‚Äù vs ‚ÄúA-04‚Äù (extra hyphen in A-04). ‚Ä¢ Elevation units: ‚Äú95m‚Äù is labeled with ‚Äúm‚Äù while others are numeric or one has ‚Äú.‚Äù. ‚Ä¢ Missing value notation: Elevation for A-04 is ‚Äú.‚Äù, which is not standard (should be NA or blank). ‚Ä¢ Counts: NA vs blank vs 0 ‚Äì second species count for A02 is NA (probably meaning no data), whereas others have numeric including 0 (which might mean truly zero count). We should standardize how absence vs not recorded is represented. ‚Ä¢ Notes inconsistency: A03 note has a question mark ‚Äúseedlings included?‚Äù ‚Äì unclear data collection issue. ‚Ä¢ Data structure: species counts are in separate columns (species_1_count, species_2_count). If the number of species is large or variable, a tidy format would have a separate table or a ‚Äúspecies‚Äù column. In this small example it‚Äôs okay, but it hints at a possible need to pivot longer if many species. ‚Ä¢ Units in data: ‚Äú95m‚Äù should be numeric 95 with units documented elsewhere (units shouldn‚Äôt be mixed into the data cell with the value). There may be more issues (like if 120 and 110 are presumably in meters but one included ‚Äúm‚Äù, confirm all are meters, etc.). Each would require a cleaning decision.\n\n\n\nThe exercise above highlights why cleaning can be a big job: you have to catch and fix all these things. Using R for this is advantageous ‚Äì you can write code to, say, remove the ‚Äúm‚Äù and convert columns to numeric, or unify the plot_id format (remove hyphens), and you can run this code again if new data come in with the same issues.\n\n\n\nImplementing cleaning in R\nMost data cleaning operations fall into a few categories: - Filtering rows: e.g., remove obviously erroneous records (negative counts, etc.) or maybe filter to a date range. - Recoding values: e.g., turn ‚ÄúNA‚Äù or ‚Äú.‚Äù or ‚Äú-999‚Äù into actual NA, unify ‚ÄúM‚Äù vs ‚ÄúMale‚Äù vs ‚Äúmale‚Äù into one category. - Type conversion: ensure numbers are numeric, dates are date type, etc. (Excel often messes this up, so double-check after import). - Creating new columns or dropping columns: e.g., separate a combined ‚Äúdate_time‚Äù into two fields, or drop an unused ‚Äúnotes‚Äù column for analysis (but perhaps store it elsewhere). - Reshaping or merging as discussed to get data in the right shape. We won‚Äôt cover the entire dplyr or tidyr toolkit here, but as an example: library(dplyr) field_data_clean &lt;- field_data %&gt;% mutate(elevation_m = as.numeric(gsub(‚Äúm‚Äù, ‚Äú‚Äú, elevation_m)), # remove‚Äùm‚Äù and convert to numeric plot_id = toupper(gsub(‚Äú-‚Äù, ‚Äú‚Äú, plot_id)), # remove hyphen, make uppercase species_2_count = ifelse(species_2_count ==‚Äù‚Äú, NA, species_2_count)) %&gt;% # treat blank as NA rename(species_a_count = species_1_count, species_b_count = species_2_count) In a few lines, this fixes multiple issues: - Elevation:‚Äù95m‚Äù becomes 95, and ‚Äú.‚Äù would become NA as as.numeric(‚Äú.‚Äù) gives NA (with a warning). - Plot IDs: ‚ÄúA-04‚Äù becomes ‚ÄúA04‚Äù. - Blanks in species_2_count become NA. - Renamed columns for clarity. This is just illustrative ‚Äì real cleaning might be more involved ‚Äì but notice how we can chain multiple transformations. Also, each operation is documented by the code itself. If someone wonders ‚Äúhow did 95m become 95?‚Äù ‚Äì the code shows it. Crucially, we would also keep notes in a README or in code comments about assumptions (e.g., ‚ÄúWe assume blank species count means not recorded (set to NA)‚Äù).",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day04.html#quality-control-via-code",
    "href": "days/day04.html#quality-control-via-code",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "Quality control via code",
    "text": "Quality control via code\nWhen cleaning, it‚Äôs helpful to build in checks. For instance, after cleaning, you might assert: - All plot_id follow the pattern ‚ÄúA##‚Äù (using a regex, or check length). - Elevation is within a plausible range (e.g., 0 to 150 m for our area). - No duplicates in plot_id if each plot should be unique. - The number of species columns, if pivoted longer, matches a known species list count. In R, you might use stopifnot():\n\nstopifnot(all(field_data_clean$elevation_m &gt;= 0, field_data_clean$elevation_m &lt;= 150))\n\nThis will throw an error if any elevation is outside 0-150. Or simply use summary() and table() to eyeball distributions after each major step. There are also packages like validate or testthat that can formalize data testing, but simple checks with if or stopifnot statements go a long way. Another tool: if you have expected relationships (e.g., a species code must exist in the species metadata table), you can test that no species in observations is missing from the metadata:\n\nmissing_species &lt;- setdiff(unique(observations$species_code), species_info$species_code)\nif(length(missing_species) &gt; 0) {\n  warning(\"These species codes are in observations but not in metadata: \", paste(missing_species, collapse=\", \"))\n}\n\nThis would alert you to a likely mismatch to address. By encoding these checks in your script, every time you run the cleaning script, it will automatically flag issues. This is much better than manually scanning spreadsheets for problems.\n\nDocumentation and project organization\nAs you clean, comment your code. For example:\n\n# Remove 'm' from elevation and convert to numeric. Note: one entry was '.', which becomes NA.\nfield_data$elevation_m &lt;- as.numeric(gsub(\"m\", \"\", field_data$elevation_m))\n\nThis way, anyone reading the script (including you, two months later) knows why you did something. README files: A README for your data can detail: - Data source and collection methods. - What cleaning steps were done (in broad strokes). - Definitions of fields, units, codes. - Any known quirks (e.g., ‚ÄúPlot A04 was measured with a different method, values adjusted accordingly‚Äù). By including such documentation with your dataset (as a separate .md or .txt file), you ensure that context isn‚Äôt lost. This is akin to metadata you learned about on Day 1. Project structure: Keeping your work organized helps reproducibility. A common layout:\nmy_project/ data/ raw/ # original raw data (read-only, do not edit manually) temp/ # intermediate files (if needed) clean/ # cleaned data output scripts/ 01_cleaning.R 02_analysis.R outputs/ figures/ tables/ README.md my_project.Rproj\nThe idea is to separate raw data from generated results and code. The raw data is immutable (if an update comes, you add a new file or replace it but never mix with processed data). Scripts are numbered or named by stage (first cleaning, then analysis, etc.). Outputs are where plots or reports go. Using an RStudio Project (.Rproj) or otherwise setting your working directory to the project folder ensures that file paths are relative to the project (no more C:... hard-coded paths, which break on another computer). You might wanna check the package here: here::here() to manage paths easily.\nIn your scripts, you might read data like:\n\nlibrary(here)\n\nhere() starts at C:/Users/ohagen/Documents/GitHub/course_DigitalCompetenceInTheBiologicalSciences\n\nprint(getwd())\n\n[1] \"C:/Users/ohagen/Documents/GitHub/course_DigitalCompetenceInTheBiologicalSciences/days\"\n\nhere::i_am(\"data/raw/csv/Caxiuana_tree_trait_data.csv\")\n\nhere() starts at C:/Users/ohagen/Documents/GitHub/course_DigitalCompetenceInTheBiologicalSciences\n\nraw_data &lt;- read.csv(here(\"data/raw/csv/Caxiuana_tree_trait_data.csv\"))\n\nSince the working directory is the project, this path works for anyone who clones the project. Speaking of cloning‚Ä¶\n\n\nVersion control with Git\nGit is a tool that tracks changes in files over time, enabling you to recall specific versions later. Think of it as a supercharged undo/redo that works even across sessions and collaborators.\nKey concepts:\n\nRepository (repo): a folder that is tracked by Git.\nCommit: a snapshot of the state of files with a message describing the changes. E.g., ‚ÄúCorrected elevation units and standardized plot IDs‚Äù.\nHistory: a sequence of commits; you can navigate through them, see diffs (what changed), who changed what (if collaborating).\nBranch: a parallel line of development. You can keep an experimental branch separate from main, then merge if it works out.\nRemote (like GitHub/GitLab): an online copy of the repo for backup or collaboration.\n\nWhy use Git for data science:\n\nIt provides backups and change history for your code (and even data, though large raw data files often better not version due to size).\nIf you break something in your script, you can compare to yesterday‚Äôs version and revert if needed.\nIt encourages small, documented changes (commits) ‚Äì which parallels documenting your work scientifically.\nCollaboration: multiple people can work on different parts of the project, merge contributions, review changes, etc.\n\nEven if working solo, using Git + a remote like GitHub means you won‚Äôt lose work if your laptop dies, and you can easily share your project (or publish it as supplementary material for a paper). For example, this course material is likely maintained in a Git repo. Every edit I (the instructor) make is tracked.\nBasic Git workflow:\n\nInitialize a repo: git init (or use RStudio‚Äôs version control features to create a new project with Git).\nStage and commit files: git add script1.R; git commit -m ‚ÄúAdded cleaning script‚Äù\nRepeat commits as you reach logical milestones.\nView history: git log (or GUI in RStudio/GitHub Desktop).\nUndo/branch/merge as needed (more advanced, but incredibly useful as you get comfortable).\n\nUsing Git through RStudio: RStudio has a Git pane where you see changes, diff, commit, push/pull if a remote is set. This can lower the barrier to entry. Important: version control is primarily for code and small text data. Large raw datasets are often kept outside Git (you might store them elsewhere and just version a small subset or a pointer). But your cleaning scripts and output summaries (which are text or code) fit well into version control.\n\n\nReproducible reports with Quarto/R Markdown\nBy now, you have code to clean and analyze data. The final step of any project is often to communicate results (figures, tables, conclusions). Quarto (the tool used to create these course notes) allows us to keep code and narrative in one document. This means:\n\nThe figures and stats in your report are generated by the actual code in that report, so they are always up-to-date with the latest data and analysis.\nYou can output to multiple formats (HTML web page, PDF article, Word doc, slides, etc.) from the same source file.\nIt encourages writing about what you‚Äôre doing as you do it, which is great for reflection and for others reading it.\n\nA Quarto document (.qmd) or R Markdown (.Rmd) has chunks of code interwoven with Markdown text. For instance: We found that the average DBH was 26.1289773 meters (n = 176 plots).\n\nhist(trees$height, main=\"Tree Height Distribution\", xlab=\"Height (m)\")\n\nIn the text above, r expression inserts the result of an R expression inline (like the mean). The chunk below it runs R code to produce a histogram (with echo: false meaning the code is hidden, only the plot shows). When you render this file (by clicking Render or using quarto render command), it will execute the code and insert the results in the final output.\nThe beauty: if the data updates or you change a parameter, you re-render and all numbers and plots update automatically. No more copy-pasting results from R into Word manually (which is error-prone).\nQuarto is a next-gen version of R Markdown with support for multiple languages. We focus on R, but Quarto can also run Python, etc. under the hood.\nTo tie it together:\n\nUse scripts (.R files) for extensive data wrangling or exploratory work.\nWhen you‚Äôre ready to create a report or publication, use a Quarto doc to perform the final analysis and generate output, with prose explaining each result.\n\nThis course website is an example: all five days are written in Quarto, combining explanation (what you‚Äôre reading) and code (to generate any figures, etc.). It‚Äôs fully reproducible ‚Äì you could run the Quarto files and regenerate the entire site.\n\n\n\n\n\n\nTip\n\n\n\nTip: Start small with reproducible docs. Maybe begin by using R Markdown for a simple analysis write-up or lab report, to get used to it. Over time, you‚Äôll see the immense benefit of having everything in one place.\n\n\n\n\nAutomation and beyond\nWith clean data and scripted analysis, you can automate tasks:\n\nRe-run everything with new data by a single command (great for recurring reports).\nSet up continuous integration (CI) to run your analysis script on a schedule or when changes occur (beyond our scope, but a cool possibility).\nShare your project (data+code) for others to reproduce or extend ‚Äì practicing true open science.",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day04.html#wrap-up",
    "href": "days/day04.html#wrap-up",
    "title": "Day 4 ‚Äì Cleaning data and making work reproducible",
    "section": "Wrap-up",
    "text": "Wrap-up\nToday was about instilling rigor and order into the chaotic part of data analysis. We learned that cleaning data is not just pressing a few Excel buttons ‚Äì it‚Äôs a process that benefits from careful thought, and coding it in R makes it reproducible and less error-prone. We saw the tidy data framework as a guide to structuring datasets, and we practiced thinking about common data issues and how to fix them in a systematic way.\nWe also stepped back to look at the workflow as a whole: organizing files, using version control to track our progress, and using tools that knit everything together (like Quarto). These practices might require some setup and learning, but they pay off by making our work more reliable and easier to collaborate on.\nIn the Belize scenario, this means our biodiversity data, once cleaned and properly documented, can be confidently used to derive insights, knowing that anyone could inspect our process (via scripts and Git history) and repeat the analysis themselves. We are essentially safeguarding the scientific integrity of our results by being transparent and reproducible.\nTomorrow, in Day 5, we‚Äôll take a higher-level view to discuss how the tools (Excel, R, etc.) and models we choose influence our science, and reflect on everything we‚Äôve learned. We‚Äôll also address any remaining questions and discuss where to go next in your data journey.\nBy mastering the workflow up to this point, you‚Äôve significantly upgraded your digital competence as a scientist ‚Äì you can wrangle data, analyze it, and ensure that your work can be trusted and extended by others.\n\n\n\n\n\n\n\nReflection\n\n\n\n\nThink of a dataset you‚Äôve worked with before: what was one issue that caused a headache, and how might you tackle it now with what you learned (tidy data, scripting, etc.)?\nWhich tool or practice introduced today are you most likely to continue using (e.g., writing a README, using Git for your scripts, Quarto for reports)? Why?\nDo you foresee any challenges in convincing colleagues to adopt more reproducible workflows? How might you advocate for these practices?\n\nPlease share your feedback for today‚Äôs session:\n- Anonymous feedback",
    "crumbs": [
      "Course Days",
      "Day 4 ‚Äì Cleaning data and making work reproducible"
    ]
  },
  {
    "objectID": "days/day02.html",
    "href": "days/day02.html",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\ncalculate and interpret basic summary statistics (mean, median, variance, etc.)\ndescribe the variability and distribution of a dataset\nidentify how outliers and skewness affect summary statistics\ncreate simple visualizations and recognize misleading plots\nunderstand the limitations of manual analysis for repetitive tasks\nimport data into R and compute summaries and plots programmatically\nsafely import/export data from R (e.g.¬†to CSV) with correct encoding\n\nNo prior programming experience is required (we will start using R from scratch).",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#learning-goals",
    "href": "days/day02.html#learning-goals",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\ncalculate and interpret basic summary statistics (mean, median, variance, etc.)\ndescribe the variability and distribution of a dataset\nidentify how outliers and skewness affect summary statistics\ncreate simple visualizations and recognize misleading plots\nunderstand the limitations of manual analysis for repetitive tasks\nimport data into R and compute summaries and plots programmatically\nsafely import/export data from R (e.g.¬†to CSV) with correct encoding\n\nNo prior programming experience is required (we will start using R from scratch).",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#motivation",
    "href": "days/day02.html#motivation",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "Motivation",
    "text": "Motivation\nAfter structuring data properly, the next step is to make sense of it. Large tables of numbers are hard to interpret by eye. Descriptive statistics and visualizations help us summarize key properties of the data. These summaries can reveal patterns (or problems) in our Belize biodiversity dataset, guiding further analysis. Today we learn how to compute and use these summaries, first with simple tools (like Excel) and then by transitioning to R.",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#agenda",
    "href": "days/day02.html#agenda",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "Agenda",
    "text": "Agenda\n13:30‚Äì14:15 | Block 1 ‚Äì Descriptive statistics fundamentals\n- Recap of Day 1 data (Belize case) and need for summarizing\n- Measures of central tendency (mean, median and mode) and variability (range, variance)\n- Effects of outliers and skewed data on summaries\n- Understanding distributions (normal vs skewed, etc.)\n14:15‚Äì14:30 | Break ‚òïÔ∏è\n14:30‚Äì15:15 | Block 2 ‚Äì Visualizing data effectively\n- Visualizations as summaries: ‚Äúa picture is worth a thousand numbers‚Äù\n- Examples of honest vs misleading plots\n- Best practices for clear charts (axes, labels, zero-baselines)\n- Repetitive analysis tasks: Excel limitations and the need for automation\n- Transition: why use R for data analysis?\n15:15‚Äì15:30 | Break ‚òïÔ∏è\n15:30‚Äì16:15 | Block 3 ‚Äì Hands-on: first steps in R\n- Introduction to R and RStudio interface\n- Importing a CSV dataset into R\n- Computing summary statistics in R (e.g.¬†mean, median, mode, SD)\n- Creating a simple plot in R (histogram or boxplot)\n- Exporting results (writing a CSV file)\n- Discussion: comparing R workflow to Excel\n16:15‚Äì16:30 | Reflection, discussion, and outlook üß†\n- Which summary or visualization was most insightful?\n- How did using R compare to using Excel?\n- What would you like to explore next with the data?\n- Preview of Day 3: introducing programming concepts in R",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#from-raw-data-to-summary-statistics",
    "href": "days/day02.html#from-raw-data-to-summary-statistics",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "From raw data to summary statistics",
    "text": "From raw data to summary statistics\nAfter data collection we end up with a raw dataset. The first question is: what do all these numbers tell us? We need to condense the data into interpretable quantities without reading every value.\nDescriptive statistics are numbers that summarize certain aspects of the data. They help describe ‚Äútypical‚Äù values, spread of the data, and unusual observations. Crucially, these summaries simplify the data and hide detail, so they must be used carefully.\n\nMeasures of central tendency\nThe mean (aka average) and median are two common measures of the ‚Äúcenter‚Äù of a numeric dataset:\n\nMean (\\(\\bar{x}\\)): the sum of all values divided by the number of values. For data points \\(x_1, x_2, ..., x_n\\), the mean is\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.\\]\nExample: If five trees have heights (in m) 8, 10, 6, 9, 7, the mean height is \\((8+10+6+9+7)/5 = 40/5 = 8\\) m.\nMedian: the middle value when data are sorted from smallest to largest. If \\(n\\) is odd, the median is the middle data point; if \\(n\\) is even, it is the average of the two middle values.\nExample: For the tree heights 6, 7, 8, 9, 10 (sorted order), the median is 8 m (the third value). If we had an even number of trees, say heights 6, 7, 8, 10 (four values), the median would be the average of the two middle values (7 and 8), which is 7.5 m.\n\nThe mean and median both tell us about the ‚Äúcentral‚Äù value of the data, but they respond differently to extreme values. The mean is influenced by outliers (very high or low values), whereas the median is more robust in skewed distributions.\n\nMode: the most frequently occurring value in the dataset. This is less commonly used for continuous data but can be informative for categorical data (e.g., most common species observed).\n\n\nVisualisation mode median mean\n\n\n\nGeometric visualisation of the multiple statistics\n\n\n\nSource: Wikimedia Commons\n\n\n\n\n\n\n\n\nSolo Exercise: Calculate the mean (aka average), median and mode for three distributions (7 min)\n\n\n\n\nOpen one of the regionalized files in Excel (three numeric columns A, B, C, 100 rows each):\n\n\ndata/raw/csv/dist_day02_sep_semicolon__dec_comma.csv (German settings: semicolon separator, comma decimal) ‚Üí loads cleanly if your decimal is ,\ndata/raw/csv/dist_day02_sep_comma__dec_dot.csv (US/UK settings: comma separator, dot decimal) ‚Üí if your regional decimal is ,, tell Excel to keep dot as decimal One way is over Power Query import First on Home ‚Üí Use First Row as Headers Delete the automatic conversion, did you notices it changed the values Select all columns clicking the first one, then shift and then the last one. Right Click ‚Üí Change Type ‚Üí Using Locale ‚Üí Data Type [Decimal] ‚Üí Locale English (Europe)\n\nAs we learned, the other way is to copy the data (from Notepad or Notebook++) and paste it into Excel. Then on Data ‚Üí Text to Columns ‚Üí‚Ä¶ and at the end select Advanced There, set decimal separator to .).\n\nFor each variable (A, B, C), calculate the mean, median and mode using Excel functions:\n\n\nMean: =AVERAGE(range) (German Excel: =MITTELWERT(Bereich))\nMedian: =MEDIAN(range) (German Excel: =MEDIAN(Bereich))\nMode: =MODE(range) (German Excel: =MODUS.EINF(Bereich); older versions: =MODUS(Bereich))\n\nYou can do this selecting a cell range with your mouse or typing it in (e.g.¬†A2:A101 for column A) or using named ranges.\n\nKeep it open for later exercises.\n\n\n\n\n\nMeasures of variability\nA single center value is not enough; we also need to know how spread out the data are:\n\nRange: the difference between the maximum and minimum values. It tells us the span of the data (but can be skewed by outliers).\nOutliers are extreme values that differ significantly from other observations. For example, if tree heights range from 5 m to 50 m, a tree of height 150 m can be considered an outlier.\nVariance (\\(s^2\\)) measure the typical deviation of data points from the mean. Variance is defined as\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2,\\]\nStandard Deviation (\\(s\\)) the square root of the variance. It is in the same units as the data and gives a sense of typical deviation from the mean. \\(s = \\sqrt{s^2}\\). A high standard deviation means values are widely scattered around the mean; a low standard deviation means they are tightly clustered.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that standard deviation is the square root of variance. This changes the units:\n\nVariance has squared units: e.g.¬†m(^2), mm(^2), individuals(^2)\nStandard deviation has the same units as the data: e.g.¬†m, mm, individuals\n\nWhen interpreting, standard deviation is often more intuitive because it is in the same units as the original data. Variance, however, is harderd to interpret directly bacause of squared units but is algebraically convenient and often used in statistical modeling.\nVariance has clean additivity (under independence):\n[ (X+Y) = (X) + (Y) X Y ]\nStandard deviations do not add:\n[ + ]\nThat is why models tend to run on variance, not SD.\n\n\n\n\n\n\n\n\nSolo Exercise: Calculate variance and standard deviation for the three distributions (5 min)\n\n\n\n\nUsing the same dataset (pick the same file as above)\nFor each variable (A, B, C), calculate the variance and standard deviation using Excel functions:\n\n\nVariance: =VAR.S(range) (German Excel: =VARI.S(Bereich))\nStandard Deviation: =STDEV.S(range) (German Excel: =STABW.S(Bereich))\nRange: =MAX(range)-MIN(range) (German Excel: =MAX(Bereich)-MIN(Bereich))\n\n\nKeep it open for later exercises.\n\nWhay did this show you more about the data?\n\n\n\nPercentiles/Quartiles: values that divide the data into 100 (percentiles) or 4 (quartiles) equal parts. The 25th percentile (Q1) is the value below which 25% of data fall; the 75th percentile (Q3) is the value below which 75% of data fall. Median is the 50th percentile (Q2).\nInterquartile Range (IQR): the difference between the 75th percentile (Q3) and 25th percentile (Q1). This focuses on the spread of the middle 50% of data, giving a robust spread measure less affected by outliers.\n\nFor example, if the diameter at breast height (DBH) of trees in Plot A have a mean of 30 cm and a standard deviation of 5 cm, most tree DBHs are around 30 cm ¬± 5 cm. In Plot B, a mean of 30 cm but standard deviation of 15 cm would imply much greater variability (some saplings and some very large trees).\n\n\n\n\n\n\nSensitivity to outliers\n\n\n\nBoth variance and SD use squared deviations, so both are sensitive to outliers.\nIf robustness matters, prefer: - IQR (interquartile range) - Quantiles\nP.S.: robustness meaning ‚ÄúHow strongly a statistic reacts to small violations of its assumptions, especially the presence of outliers, skewness, or heavy tails‚Äù Variance and standard deviation use squared deviations. Squaring is a megaphone. One extreme value gets amplified enormously.\n\nCommon mistakes\n\nReporting variance in reader-facing summaries\nTreating mean () SD as a confidence interval (it is not)\nComparing SDs across groups with very different distributions and pretending it is apples-to-apples\nUsing SD for strongly skewed data without also reporting quantiles\n\n\n\n\n\n\nThe effect of outliers and skewness\nOutliers (values that are very high or low relative to the rest) can heavily distort certain statistics. In ecology, suppose most trees in a sample are under 10 m tall, but one giant ceiba tree is 50 m tall. The mean tree height will be pulled upward by this one extreme value, whereas the median will barely change (since the median only cares about the middle-ranked value).\nIn our Belize forest data, if one plot had an anomalously high tree height count due to an observational/data entry error, the mean count would give a misleading impression of typical conditions. Always consider whether outliers are errors, natural rare events, or indications of underlying heterogeneity. Depending on context, you might report the median instead of the mean for skewed data, or even remove obvious data errors.\nTo gauge skewness and outliers, it‚Äôs useful to look at the whole distribution of the data, not just single summary numbers. This brings us to visualizing data.",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#common-mistakes",
    "href": "days/day02.html#common-mistakes",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "Common mistakes",
    "text": "Common mistakes\n\nReporting variance in reader-facing summaries\nTreating mean () SD as a confidence interval (it is not)\nComparing SDs across groups with very different distributions and pretending it is apples-to-apples\nUsing SD for strongly skewed data without also reporting quantiles",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#distributions-and-visual-summaries",
    "href": "days/day02.html#distributions-and-visual-summaries",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "Distributions and visual summaries",
    "text": "Distributions and visual summaries\nA distribution describes how frequently each value (or range of values) occurs in the dataset. Two datasets can have the same mean and variance yet be distributed very differently. Plotting the data often reveals patterns (multiple peaks, skew, outliers) that summary statistics alone might miss.\nFor example, consider Anscombe‚Äôs quartet ‚Äì four different datasets that have the exact same mean, median, and variance, but very different arrangements of points when graphed. A visual comparison makes the differences obvious:\n\nFour datasets with identical summary statistics (mean, variance, correlation, etc.), but very different patterns when plotted. This classic example (Anscombe 1973) shows why visualizing data is crucial: summary statistics alone can be insufficient\n\n\n\nAnscombe‚Äôs quartet\n\n\n\nSource: Woodside et al 2016\n\n\n\nThe role of visualization\nGraphs provide a shorthand for understanding data. By plotting our Belize dataset, we can quickly see trends and anomalies. Common plot types for quantitative data include:\n\nHistograms: show the distribution of a single numeric variable by grouping values into bins.\nBox plots: show median, quartiles, and outliers, giving a quick summary of distribution and spread.\nScatter plots: show relationships between two numerical variables (e.g.¬†tree height vs.¬†tree diameter).\nBar charts: summarize categorical data (e.g.¬†mean value per category).\n\n\n\n\n\n\n\nNote\n\n\n\nData format and plotting workflows in Excel\n\nHistogram\nDistribution of a single numeric variable\n\nSelect the numeric column (e.g.¬†DBH_cm)\nInsert ‚Üí Statistic Chart ‚Üí Histogram (German Excel: Einf√ºgen ‚Üí Diagramme ‚Üí Statistikdiagramm ‚Üí Histogramm)\nRight-click x-axis ‚Üí Format Axis\n\nSet bin width or number of bins\n\n\n\n\nBox plot\nMedian, quartiles, and outliers\n\nSelect the numeric column\nInsert ‚Üí Statistic Chart ‚Üí Box & Whisker (German Excel: Einf√ºgen ‚Üí Diagramme ‚Üí Statistikdiagramm ‚Üí Kastendiagramm (Box & Whisker))\nExcel applies Tukey‚Äôs rule automatically. Meaning: ‚Äì The box spans the interquartile range (Q1 to Q3) ‚Äì The whiskers extend to the most extreme values within Q1-1.5IQR and Q3+1.5IQR ‚Äì Outliers are any points beyond the whiskers\n\n\n\n\n\n\n\n\n\n\nSolo Exercise: Plot a historgram and a boxplot for each if the three distributions (10 min)\n\n\n\n\nUsing the same dataset (pick the same file as above that matches your regional separators/decimals)\nFor each variable (A, B, C), create a histogram using Excel:\n\nSelect the column\n\n\n\nInsert ‚Üí Statistic Chart ‚Üí Histogram (German Excel: Einf√ºgen ‚Üí Diagramme ‚Üí Statistikdiagramm ‚Üí Histogramm)\nAdjust bin width if needed (e.g.¬†to 2) via right-click on x-axis ‚Üí Format Axis\n\nMuch easier to see the distribution this way, right?\n\n\n\n\n\n\n\n\nScatter plot\n\n\n\nRelationship between two numeric variables\n\nSelect both numeric columns\nInsert ‚Üí Scatter ‚Üí Scatter with only markers (German Excel: Einf√ºgen ‚Üí Punkt (X Y) ‚Üí Punkte (nur Marker))\nAdd axis labels and optional trendline\n\n\n\n\n\n\n\n\n\nGroup exercise: Do a scatter plot of DBH and Height (7 min)\n\n\n\n\nOpen a new Excel instance\nLoad the Belize dataset data/raw/PSP/PSP_NPD847_OH.xlsx Before plotting, think, how should a scatter plot of DBH vs Height look like? What relationship do you expect?\nSelect the columns DBH and Height\nInsert ‚Üí Scatter ‚Üí Scatter with only markers\n\nBonus 1. Make this plot at the Analysis sheet (not raw data) 2. Add a trendline via right-click on a point ‚Üí Add Trendline‚Ä¶ 3. Choose Linear and logarithmic options 4. Add equation and R-squared value to the chart\n\nR-squared (R¬≤) is a statistical measure of how well a regression model fits your data. It answers the question: ‚ÄúHow much of the variation in the outcome (Y) can be explained by the predictor(s) (X)?‚Äù If 0 model explains none of the variation, if 1.0 it explains all the variation (perfect fit) Note A high R¬≤ does not mean the model is good‚Ä¶ in biology, natural variation is high. R¬≤ ignores bias and overfitting. Use diagnostics and validation too.\n\nWhat is the relationship between DBH and Height in this dataset? Does it match your expectation?\n\n\nA good visualization summarizes the data (like statistics do) but in a more intuitive way. It is not ‚Äúproof‚Äù of a hypothesis by itself, but it helps us explore and communicate what the data are saying.\n\n\nHonest versus misleading plots\nNot all charts are created equal. It‚Äôs easy to mislead with visuals, intentionally or not. One common pitfall is manipulating the axes to exaggerate or downplay differences or showing only a narrow range on the y-axis can make a tiny difference look huge.\n\nMisleading Data Visualizations: From mistakes to fraud, bad graphs can distort the truth.\n  \n\nSource: Ana Kin 2023 - Medium\n\n\nWhen creating plots, follow these best practices: - Start axes at zero for bar charts (so heights are proportional to values). Only truncate axes if necessary for clarity, and clearly indicate breaks if used. - Label axes and units clearly. A viewer should know what the x and y values represent. - Avoid ‚Äúchart junk‚Äù ‚Äì unnecessary 3D effects, excessive colors, or anything that distracts from the data. - Use appropriate plot types: e.g.¬†don‚Äôt use a pie chart if you have many categories (it becomes hard to read). - Be honest with visual encodings: for example, if doubling area in a pictogram, ensure area truly corresponds to value (our brains interpret area non-linearly, so this can mislead).\nIn Excel, creating a basic chart (Insert ‚Üí Chart) is straightforward, but be cautious: Excel might auto-format charts in ways that aren‚Äôt ideal (e.g., truncated axes or confusing legends). Always review and tweak the default settings to ensure clarity and honesty.\n\n\n\n\n\n\nWarning\n\n\n\nBeware of accidental distortion: Even well-meaning choices, like using a logarithmic scale or excluding outliers for clarity, can confuse or mislead if not communicated. Always caption or annotate your plots to make such choices clear.\n\n\n\n\n\n\n\n\nTip\n\n\n\nGroup exercise: Visualize data in Excel (10 minutes)\nUsing the Belize and Amazon dataset create: - One histogram - One box plot - One scatter plot - One bar chart (using a PivotTable)",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#when-excel-falls-short-repetition-and-scalability",
    "href": "days/day02.html#when-excel-falls-short-repetition-and-scalability",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "When Excel falls short: repetition and scalability",
    "text": "When Excel falls short: repetition and scalability\nIf you have a small dataset, you can calculate means or make a plot in Excel by hand without much trouble. But what if you have dozens of variables or need to repeat the same calculation for multiple groups? This is where Excel becomes tedious and error-prone, and why we turn to R.\nImagine you have species count data for 50 different areas and you want the average count for each species in each area. In Excel, you might set up a complex grid of formulas or create 50 separate pivot tables ‚Äì a process that is both time-consuming and fragile (any new data or slight change, and you must update everything manually). In R, you could compute all those summaries with a few lines of code that can be reused anytime the data updates.\nReproducibility: Another limitation of point-and-click analysis is that it‚Äôs hard to retrace or repeat exactly. If someone asks ‚ÄúHow did you get this number?‚Äù, you might have to recall a sequence of steps you did in Excel. In R, every step is scripted, so you (and others) can always check the code to see how a result was obtained and rerun it on new data.\nAccuracy: Manual work is prone to mistakes (copy-paste errors, wrong cell ranges, etc.). By writing an explicit script, you reduce the risk of such errors, especially when tasks are repetitive.\nExcel does have tools like PivotTables and formulas that can automate some analyses. PivotTables, for example, allow quick grouping and summarizing of data by categories (e.g.¬†mean count per species per site). We used some Excel features on Day 1 to structure data. However, for complex or repetitive tasks, maintaining many pivots and formulas can become a nightmare of cross-references and as we saw, error susceptible. R, by contrast, is great at handling repetition: you can loop through all species or use vectorized operations to compute results for all groups in one go.\nIn summary, Excel is often the easiest for one-off calculations or small tasks, but R is better for automating analyses, handling larger datasets, and ensuring reproducibility. Next, we‚Äôll get hands-on with R and perform some of the same summaries and plots we did in Excel ‚Äì but faster and in a way that scales up.",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#first-steps-in-r",
    "href": "days/day02.html#first-steps-in-r",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "First steps in R",
    "text": "First steps in R\nR is a programming language and environment specifically designed for data analysis and statistics. We will use R through RStudio, a user-friendly interface.\nTo follow along:\n\nOpen RStudio. You‚Äôll see the Console in one pane and possibly an empty script editor in another. You can type R commands directly into the Console, or write a script in the editor pane and run it.\nCreate a new R script: Go to File ‚Üí New File ‚Üí R Script. This allows you to save a record of your commands (which is great for reproducibility).\n\n\nImporting a dataset into R\nWe will start by importing a CSV file of data (for example, the tree measurement dataset we structured on Day 1). In Excel, we opened this file by double-clicking or via Data ‚Üí Import. In R, we use a function call.\nAssuming the CSV file is saved as data/raw/trees.csv (just as an example path), we can read it into R with:\n# Read the CSV file into a data frame (tabular structure in R)\ntrees &lt;- read.csv(\"data/raw/csv/Caxiuana_tree_trait_data.csv\")\nThis command loads the data into a variable named trees. By default, read.csv assumes the file has a header row for column names, values are separated by commas, and text is encoded in UTF-8 (which is standard for most CSVs). If our data used a different delimiter (like tab \\t in a TSV file), we could use read.delim or specify sep=‚Äú\\t‚Äù. One big advantage: R will not automatically mangle our data the way Excel might. For instance, if there were gene names like ‚ÄúSEP2‚Äù or ‚ÄúMAR1‚Äù, R will keep them as text, whereas Excel might have turned them into dates or numbers. In R, data types are explicit ‚Äì we can tell R a column is text, numeric, etc., or let it guess (which it usually does correctly if the file is well-formatted).\n\n\n\n\n\n\nTip\n\n\n\nTip: If your data contains special characters (like ‚Äú√ü‚Äù or ‚Äú√±‚Äù), ensure you use the correct encoding. read.csv(‚Ä¶, fileEncoding=‚ÄúUTF-8‚Äù) will handle most cases. This prevents issues where characters turn into gibberish due to encoding mismatches.\n\n\n\n\nSummarizing data in R\nNow that our data is in R as a data frame (trees in the example), we can compute the same summary statistics as before:\n\n# Basic summaries\nmean(trees$DBH)      # mean of tree diameters\nmedian(trees$DBH)    # median\nsd(trees$DBH)        # standard deviation\nrange(trees$DBH)     # min and max\nquantile(trees$DBH)  # quartiles\n# Basic plots\nhist(trees$DBH)      # histogram of diameters\nboxplot(trees$DBH)   # boxplot of diameters\nplot(trees$DBH, trees$Mean.Growth)  # scatter plot of DBH vs growth\n\nThe functions mean(), median(), sd() (standard deviation), range() and others do the math for us. By default, these will return NA (Not Available) if any missing values are in the data. To ignore missing values (NA) in calculations, we add an argument like na.rm=TRUE (e.g., mean(trees$dbh_cm, na.rm=TRUE)). For a quick overview of an entire data frame, R has the handy summary() function.\n\n# mean of vector with NA\nvect &lt;- trees$Mean.Growth\nmean(vect)\nmean(vect, na.rm=TRUE)  # ignore NAs\n# Summary of all columns\nsummary(trees)\n\nsummary(trees) outputs, for each column, some summary info (for numeric columns: min, Q1, median, mean, Q3, max; for factors or characters: a preview of values or counts). It‚Äôs an instant way to see the shape of your data. If we have categorical variables (e.g.¬†species or site name), we might want to see counts or group-wise summaries. In R, it‚Äôs easy to do by grouping the data. For instance, to get the mean dbh_cm per species:\n\n# mean DBH per species\nmeanDBH &lt;- aggregate(DBH ~ Species, data = trees, FUN = mean)\n# report number of observations per species\ncount &lt;- aggregate(DBH ~ Species, data = trees, FUN = length)\n# merge summaries\nsummary_table &lt;- merge(meanDBH, count, by=\"Species\")\ncolnames(summary_table) &lt;- c(\"Species\", \"Mean_DBH\", \"Count\")\n# exclude species with less than 5 observations\nsubset_summary_table &lt;- subset(summary_table, Count &gt;= 5)\n#or alternatively\nsubset_summary_table &lt;- summary_table[summary_table$Count &gt;= 5, ]\n# or alternatively using dplyr package\ninstall.packages(\"dplyr\")  # run this line once to install dplyr\nlibrary(dplyr)\nsubset_summary_table &lt;- filter(summary_table, Count &gt;= 5)\n# or alternatively\nsubset_summary_table &lt;- summary_table |&gt;  filter(Count &gt;= 5)\n\nThis single line replaces what might be a complex PivotTable in Excel, giving us a table of mean diameters for each species in our dataset. We‚Äôll explore more powerful grouping and summarizing (with the dplyr package) on later days, but this shows the idea.\n\n\n\n\n\n\nGroup exercise: Summarize and plot in R (15 min)\n\n\n\nWork in pairs (one ‚Äúdriver‚Äù at the keyboard, one ‚Äúnavigator‚Äù guiding):\n\nLoad the dataset csv/Caxiuana_tree_trait_data.csv into R.\n\nAlternatively, load the Excel file Foresty/PLOT_Data_2020/PLOT_2016.10_2020.xlsx into R. For this, you will need a package, i.e.¬†openxls to load a .xlsx file, specifically the read.xlsx() function.\n\ninstall.packages(\"openxlsx\")  # run this line once to install openxlsx\nlibrary(openxlsx)\nfplot &lt;- read.xlsx(\"./data/raw/Foresty/PLOT_Data_2020/PLOT_2016.10_2020.xlsx\",sheet=1, startRow=9)[-1,]\n\nTry to figure out what this is doing!\n\nUsing R, calculate the mean and median of a key variable across all observations.\nCreate a histogram and a boxplot.\nDiscuss with your partner: Do the mean and median differ notably? What does that tell you about the distribution (skewness or outliers)? Did you intuition matched what you saw on the histogram?\n\nSwitch roles (driver/navigator) halfway through. This exercise gives you first-hand experience with R‚Äôs commands. Don‚Äôt worry if you get stuck ‚Äì use ?functionName (e.g.¬†?mean) to access R‚Äôs help, or ask an instructor for a hint. Soon we will dive deeper\n\n\n\n\nExporting data and results\nJust as important as reading data into R is writing it out. If you cleaned or summarized data in R, you might want to save it as a CSV to share or to use in a report:\n\nwrite.csv(trees, \"data/clean/trees_clean.csv\", row.names = FALSE)\n\nThis would save our trees data frame to a new CSV file (without including row numbers as an extra column). By default, write.csv writes in UTF-8 encoding on most systems, which will preserve special characters. R can also save plots to files (PNG, PDF, etc.). Another powerful approach is to use R Markdown or Quarto (as we are doing for these course materials) to integrate text, code, and plots in one reproducible document. We will delve into that on Day 4. For now, the key point is: with a few lines of code, we imported data, computed summaries, made plots, and saved outputs. Once written, we can reuse this code anytime with new data, ensuring consistent analysis. This is a taste of what‚Äôs possible as we move further into R.",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day02.html#wrap-up",
    "href": "days/day02.html#wrap-up",
    "title": "Day 2 ‚Äì Describing Data",
    "section": "Wrap-up",
    "text": "Wrap-up\nToday we learned how to describe and visualize data, transitioning from manual Excel steps to automated R scripts. Descriptive statistics like mean, median, and standard deviation condense data into understandable numbers, while visualizations reveal patterns and anomalies that numbers alone might hide. We also saw the first glimpses of R‚Äôs capabilities ‚Äì from reading data to making a plot in seconds. By summarizing the Belize case study data, we gained insight into typical values and variability (e.g., average tree sizes, distribution of observations across sites). We also confronted how misleading it can be if we rely on a single number or a poorly crafted chart. In the next session, we‚Äôll build on this foundation by asking: How can we automate analyses and go beyond what spreadsheets allow? This will lead us deeper into the world of programming (using R) ‚Äì writing instructions to let the computer handle repetitive tasks and complex logic, which is our focus for Day 3. Feel free to practice more with R on your own: try computing stats or making charts for other variables in the dataset. The more you play with the data, the more familiar you will become with R.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you switch between regions: both CSV variants of dist_day02 load cleanly when the separator and decimal match your settings. In R, set sep and dec explicitly (see below) and both files produce the same numeric results. In Excel, either pick the semicolon/comma file when your decimal is ,, or if you open the comma/dot file under a comma-decimal locale, use Power Query or Data ‚Üí Text to Columns ‚Üí Advanced and set decimal separator to . so numbers are parsed correctly.\n\nlibrary(here)\n# Read CSV with different separators/decimals; both give identical data when sep/dec match\ntab &lt;- read.csv(here(\"./data/raw/csv/dist_day02_sep_comma__dec_dot.csv\"), sep=\",\", dec=\".\")\ntab &lt;- read.csv(here(\"./data/raw/csv/dist_day02_sep_semicolon__dec_comma.csv\"), sep=\";\", dec=\",\")\nstr(tab)\nbuffer &lt;- 5\nhist(tab$A, xlim=c(min(tab)-buffer,max(tab)+buffer))\nhist(tab$B, xlim=c(min(tab)-buffer,max(tab)+buffer))\nhist(tab$C, xlim=c(min(tab)-buffer,max(tab)+buffer))\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nWhich statistic or visualization surprised you?\nHow was your first experience using R? What was challenging, and what felt easier or more powerful than using Excel?\nWhat would you like to explore next with R or data analysis? If you can, please take a moment to provide feedback on today‚Äôs session:\nAnonymous feedback",
    "crumbs": [
      "Course Days",
      "Day 2 ‚Äì Describing Data"
    ]
  },
  {
    "objectID": "days/day01.html",
    "href": "days/day01.html",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\nexplain what data represents and what it does not\ndistinguish data lifecycles (stages) e.g.¬†data entry, data storage, data analysis\nrecognize common sources of mistakes\nlearn core principles of data handling and management\nuse Excel to enter and structure biological data in a controlled way\nunderstand that every dataset is a simplified representation of reality\n\nNo prior experience with Excel or programming is required.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#learning-goals",
    "href": "days/day01.html#learning-goals",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\nexplain what data represents and what it does not\ndistinguish data lifecycles (stages) e.g.¬†data entry, data storage, data analysis\nrecognize common sources of mistakes\nlearn core principles of data handling and management\nuse Excel to enter and structure biological data in a controlled way\nunderstand that every dataset is a simplified representation of reality\n\nNo prior experience with Excel or programming is required.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#motivation",
    "href": "days/day01.html#motivation",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Motivation",
    "text": "Motivation\nBiology deals with complex, variable systems.\nData are not reality.\nData are a structured representation of selected aspects of reality.\nBefore we analyze anything, we must understand:\n\nwhat was observed,\nhow it was recorded,\nand what information was lost in the process.\n\nPoorly structured data cannot be rescued by sophisticated analysis later.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#agenda",
    "href": "days/day01.html#agenda",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Agenda",
    "text": "Agenda\n\nCourse overview and expectations\nThe data lifecycle (high level)\nWhat Excel is good at ‚Äî and where it fails\nReal-world examples of silent data corruption\nReflection and outlook to Day 2\n\n13:30‚Äì14:15 | Block 1 ‚Äì Introduction and course framing\n\nCourse is structured (5 days, Excel ‚Üí R)\nHow we will work: alone and in pairs, hands-on, reflection\nWhere materials, data, and exercises live (course website / GitHub)\nShort introduction: who I am\nBelize protected-area case study\n\n14:15‚Äì14:30 | Break ‚òïÔ∏è\n14:30‚Äì15:15 | Block 2 ‚Äì What data is (and what it is not)\n\nOverview of the data lifecycle and where Excel fits in\nFAIR: Findability, Acessibility, Interoperability, and Reusability\nExamples of spreadsheet failures\nData vs reality: observations, variables, metadata\nData as a simplified representation of complex systems\nGood vs bad tables: diagnosing common problems\nWhy structure matters more than software\n\n15:15‚Äì15:30 | Break ‚òïÔ∏è\n15:30‚Äì16:15 | Block 3 ‚Äì Hands-on: structured data entry in Excel\n\nRules for analysis-ready tables\nEnter biodiversity monitoring data using a provided template\nWork in pairs (driver / navigator)\nIdentify and discuss emerging inconsistencies\n\n16:15‚Äì16:30 | Reflection, discussion, and outlook üß†\n\nWhat was harder than expected?\nWhich problems cannot be fixed later?\nWhat questions remain open?\nPreview of Day 2: summarizing and visualizing data",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#context-a-real-world-case-study",
    "href": "days/day01.html#context-a-real-world-case-study",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Context: a real-world case study",
    "text": "Context: a real-world case study\nThroughout this course, we work with a simplified but real world scenario:\n\nYou are working with biodiversity monitoring, conservation planing and foresty data from a national protected area in Belize.\nData are collected by yourself, rangers, partner NGOs, and external consultants over long periods of time.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#what-is-data",
    "href": "days/day01.html#what-is-data",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "What is data?",
    "text": "What is data?\nDeliberately tool-agnostic and discipline-neutral.\n\n\n\n\nShort version\n\n\nData are recorded observations about the world.\n\n\n\n\n\n\nLonger version\n\n\nData are representations of observations, measurements, or events, encoded in a form that allows storage, comparison, and interpretation.\n\n\n\n\n\nData are not facts themselves, they are encodings of facts.\n\nThis distinction matters: tools only operate on the encoding, never on reality.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#core-comcepts-definition",
    "href": "days/day01.html#core-comcepts-definition",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Core comcepts definition",
    "text": "Core comcepts definition\nIn this course, we use the following working definitions:\n\nObservation: something you perceive or record in the world\ne.g.¬†one species observation at one location and time\nMeasurement: an observation made using a rule or instrument\ne.g.¬†‚ÄúTree height = 12.4 m measured with a clinometer.‚Äù\nVariable: a property that can vary between observations\ne.g.¬†species name, height, location, date, time, count\nData: the recorded representation of that measurement\ne.g.¬†a value in a table: height = 12.4\nDataset: a collection of observations recorded in a structured form e.g.¬†a spreadsheet or CSV file with many rows and columns\nMetadata: information about how, when, where, and why data were collected e.g.¬†units, methods, location, observer, context\n\nOnce written down, data are detached from the original context unless you preserve it.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#what-counts-as-data-in-biology",
    "href": "days/day01.html#what-counts-as-data-in-biology",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "What counts as data in biology?",
    "text": "What counts as data in biology?\nIn biology, data can take many forms:\n\nnumbers (e.g.¬†body mass, abundance, concentration)\ncategories (e.g.¬†species name, habitat type)\ntext (e.g.¬†field notes, descriptions)\nimages (e.g.¬†microscopy, camera traps)\nsequences (e.g.¬†DNA, RNA, proteins)\ntime series (e.g.¬†temperature, population size)\nspatial coordinates (e.g.¬†GPS locations)\n\nKey point\nData are always abstractions of reality, never reality itself.\nThis is why interpretation without context is dangerous and wrong.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#why-context-matters-metadata",
    "href": "days/day01.html#why-context-matters-metadata",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Why context matters: metadata",
    "text": "Why context matters: metadata\nData without context are dangerous.\nThat context is called metadata, for example:\n\nunits\n\nmethods\n\ntime\n\nlocation\n\nwho collected it\n\nhow it was collected\n\nwhat assumptions were made\n\n\nExample: There is data, and there is data\n\n\n\n\n\n\n\nRecorded value\nMeaning\n\n\n\n\n12.4\nmeaningless\n\n\n12.4 m\nbetter\n\n\n12.4 m; tree height; clinometer; dry season; treeID766; transect A; observer O. Hagen\nusable\n\n\n\nRule of thumb:\n- Data + metadata = information\n- Data without metadata = confusion\nThis is exactly why FAIR stresses rich metadata and standardization: without it, your ‚Äúdata‚Äù are not really reusable.\n\n\n\nWhere metadata should live (best practice)\nExcel sheets are bad at metadata. Do not rely on formatting or comments.\nUse three layers instead:\n\nFile name\n\nEncodes place, year, content, and status.\n\nREADME file (outside Excel)\nREADME.md or README.txt\n\nDescribe: - who collected the data - when and where - units - methods - known issues\n\nDedicated metadata sheet (inside Excel)\n\n\n\none sheet called metadata\nplain text only\nno merged cells\n\n\n\n\n\n\n\nWarning\n\n\n\nRisk of Excel-only metadata\n\nformatting is lost on export\n\nmeaning disappears outside Excel",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#the-fair-principles-what-they-really-mean",
    "href": "days/day01.html#the-fair-principles-what-they-really-mean",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "The FAIR principles (what they really mean)",
    "text": "The FAIR principles (what they really mean)\nThe FAIR principles describe how scientific data should be handled so that they remain useful beyond the moment they are collected.\nFAIR does not mean ‚Äúopen to everyone‚Äù and it does not guarantee data quality.\nFAIR means that data are prepared in a way that allows others (and your future self) to find, understand, and reuse them.\nWe present FAIR principles expanded from\nWilkinson et al.¬†(2016), The FAIR Guiding Principles for scientific data management and stewardship\nby the wider research data management community.\nFAIR stands for:\n\nüîç Findable\nWhat it means\nData can be discovered by humans and machines.\nIn practice, this requires - a clear name or identifier - descriptive metadata - a stable location (repository, database, archive)\nWhy it matters If data cannot be found, they effectively do not exist for science.\n\nA file on your laptop called final_data_v3_REAL.xlsx is not findable.\n\n\n\n\nüîì Accessible\nWhat it means\nOnce found, it is clear how the data can be accessed, even if access is restricted.\nIn practice, this requires - a clear access procedure - documented permissions or licenses - long-term availability\nWhy it matters Accessibility is about clarity, not openness.\nEven restricted data must explain how access works.\n\n‚ÄúEmail the author‚Äù is not a sustainable access strategy.\n\n\n\n\nüîó Interoperable\nWhat it means\nData can be combined with other data and used by different tools.\nIn practice, this requires - standard formats (e.g.¬†CSV, not proprietary-only files) - consistent units - controlled vocabularies - clear data types\nWhy it matters If every dataset uses its own conventions, integration becomes untractable.\n\n12, 12.0, 12 m, and twelve are not interoperable without context.\n\n\n\n\n‚ôªÔ∏è Reusable\nWhat it means\nData can be understood and used correctly in a new context.\nIn practice, this requires - rich metadata - documented methods - clear assumptions - provenance (who did what, when, and how)\nWhy it matters Reuse without context leads to misinterpretation and error.\n\nData without metadata are numbers without meaning.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#data-are-not-neutral",
    "href": "days/day01.html#data-are-not-neutral",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Data are not neutral",
    "text": "Data are not neutral\nThis is important and often missed.\nData depend on:\n\nwhat you chose to measure\n\nwhat you ignored\n\ninstrument precision\n\nhuman judgement\n\nclassification decisions\n\nIn ecology, for example:\n\nspecies presence depends on detectability\n\nabsence is rarely true absence\n\ncounts depend on effort\n\nSo data always embed theory, assumptions, and bias.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#why-this-matters-for-tools-like-excel-and-r",
    "href": "days/day01.html#why-this-matters-for-tools-like-excel-and-r",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Why this matters for tools like Excel and R",
    "text": "Why this matters for tools like Excel and R\nTools don‚Äôt understand meaning. They only manipulate symbols.\n\nExcel does not know what a species is.\n\nR does not know what a tree is.\n\nBoth only see strings, numbers, and dates.\n\nThat‚Äôs why:\n\nExcel can silently change your data (auto-conversion, formatting surprises)\n\nR forces you to be explicit (types, scripts, reproducible steps)\n\nBottom line: the responsibility for meaning stays with the scientist.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#data-as-a-model",
    "href": "days/day01.html#data-as-a-model",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Data as a model",
    "text": "Data as a model\nEvery dataset is already a model:\n\nit includes some aspects of reality\nit excludes others\nit simplifies complex systems\n\nThis idea will return later in statistics and modelling courses.\nFor now, remember:\n\nData are never neutral.\nChoices are made at the moment of data collection.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#the-data-lifecycle",
    "href": "days/day01.html#the-data-lifecycle",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "The Data lifecycle",
    "text": "The Data lifecycle\n\n    \n    \n    \n    \n    \n    \n    \n\n    \n        Data generation Real-world measurements and observations\n        Data entry & initial structuring Transcribing, defining rows/columns, first checks\n        Data cleaning & quality control Fixing typos, harmonising codes, handling missing values\n        Data storage & archiving Make data durable, portable, and shareable\n        Data analysis Statistics, models, simulations\n        Visualization & communication Figures, tables, narratives\n        Reproducibility & reuse Scripts, version control, sharing\n    \n\n    \n        \n            Data generation\n            What this stage is about\n            \n                Data originate in the real world: measurements, observations, experiments, surveys, sensors, and field notes.\n            \n            Biology examples\n            \n                Species counts, trait measurements, GPS tracks, lab assays, environmental monitoring.\n            \n            Good tools\n            \n                Field sheets, instruments, sensors, lab notebooks, electronic lab notebooks (ELNs).\n            \n        \n\n        \n            Data entry & initial structuring\n            What this stage is about\n            \n                Transcribing observations into digital form.\n                Defining rows, columns, identifiers, and units.\n                Performing the very first human sanity checks.\n            \n            Good tools\n            \n                Excel, Google Sheets, LibreOffice Calc, simple data entry forms.\n            \n            Excel: ‚úÖ Excellent when used carefully (fast manual entry, visual feedback, validation rules).\n            R: ‚ö†Ô∏è Not designed for manual entry; it expects data files or databases.\n            Key rule: one row = one observation, one column = one variable, and no analysis logic in the raw file.\n        \n\n        \n            Data cleaning & quality control\n            What this stage is about\n            \n                Fixing typos, harmonising codes, handling missing values.\n                Checking ranges, units, and consistency.\n            \n            Good tools\n            \n                Excel (for transparent, small datasets), R, Python (pandas).\n            \n            Excel: ‚ö†Ô∏è Limited and risky because changes are hard to audit and history is lost.\n            R: ‚úÖ Very strong ‚Äî every cleaning step is scripted and reproducible.\n            Key transition: if you cannot explain exactly what you changed and why, leave Excel.\n        \n\n        \n            Data storage & archiving\n            What this stage is about\n            \n                Long-term storage, collaboration, and repository submission.\n            \n            Good tools\n            \n                Plain-text tables (CSV/TSV), databases, Zenodo, Dryad, institutional servers.\n            \n            Excel: ‚ùå Poor choice ‚Äî hides metadata, embeds formulas, fragile across versions, hard to diff.\n            R: ‚ö†Ô∏è Produces clean portable data files but is not itself the archive.\n        \n\n        \n            Data analysis\n            What this stage is about\n            \n                Statistical summaries, models, hypothesis testing, simulations.\n            \n            Good tools\n            \n                R, Python, specialized statistical software.\n            \n            Excel: ‚ùå Danger zone ‚Äî formulas are opaque, errors hard to detect, reproducibility is weak.\n            R: ‚úÖ Core strength ‚Äî transparent, testable, and scriptable analysis.\n        \n\n        \n            Visualization & communication\n            What this stage is about\n            \n                Figures, tables, reports, presentations.\n            \n            Good tools\n            \n                R (ggplot2), Python (matplotlib, seaborn), Quarto, Illustrator for polishing.\n            \n            Excel: ‚ö†Ô∏è Fine for quick exploratory plots and sanity checks, but not for final reproducible figures.\n            R: ‚úÖ Excellent control and consistency.\n        \n\n        \n            Reproducibility & reuse\n            What this stage is about\n            \n                Others reusing your data, or you revisiting it months later.\n                Reviewers asking ‚Äúhow did you get this result?‚Äù\n            \n            Good tools\n            \n                Scripts, version control (Git), Quarto/notebooks.\n            \n            Excel: ‚ùå Actively hostile ‚Äî undocumented and non-repeatable.\n            R: ‚úÖ Designed for exact reruns with explicit provenance.\n        \n    \n\nExcel is widely used because it is:\n\neasy to access\nhuman-readable\nflexible\n\nExcel is well suited for: - data entry - inspection - small-scale summaries\nExcel is not well suited for: - enforcing consistency - documenting decisions - scaling analyses\nUnderstanding these limits is part of digital competence.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#spreadsheet-failures-lead-to-financial-policy-and-health-costs",
    "href": "days/day01.html#spreadsheet-failures-lead-to-financial-policy-and-health-costs",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Spreadsheet Failures lead to Financial, Policy, and Health Costs",
    "text": "Spreadsheet Failures lead to Financial, Policy, and Health Costs\nBefore we look at biological and environmental data, it is worth understanding why spreadsheets deserve respect. The cases below are not about beginners. They involve intelligence agencies, governments, banks, and elite academics.\nThese failures occurred because spreadsheets silently changed data, hid assumptions, or scaled beyond what they were designed for.\n\nWhy Excel behaves the way it does\nExcel is designed for convenience, not scientific safety.\nIt assumes: - dates are more common than gene names - numbers should be auto-formatted - users want speed, not explicitness\nThe following examples are not edge cases. They are the logical consequence of these design choices.\n\n\n\nüïµÔ∏èüìûüéØ‚ùå MI5 wiretapped the wrong people\n(UK intelligence error) Source: The Guardian\nA spreadsheet formatting issue altered the last three digits of phone numbers to 000, leading to 134 incorrect surveillance targets. The error originated from automatic formatting in a spreadsheet.\nLesson\n\nAutomatic data conversion can silently change identifiers.\n\nWhy it matters\n\nIntelligence and security systems rely on exact identifiers.\nA single formatting error can redirect surveillance to innocent people.\n\n\n\n\nü¶†üìâüö´üìã Nearly 16,000 COVID-19 cases lost in England\n(Public health reporting failure) Source: The Guardian\nDuring the COVID-19 pandemic, 15,841 positive cases were omitted from official statistics because a CSV file was opened in Excel and truncated at the row limit. (i.e.¬†65,536 for the used version). Contact tracing was delayed and incomplete as a result. Can we even calculate the true impact of these errors?\n\nIn modern Microsoft Excel (including Office 365 / Microsoft 365) the maximum number of rows per worksheet is 1,048,576. You cannot add more rows in a single sheet beyond that limit; if you try to import data that exceeds this, rows beyond the limit may be truncated. The maximum number of columns is 16,384 (column XFD Support Microsoft).\n\nLesson\n\nExcel has hard limits (row counts) that are invisible until exceeded.\n\nWhy it matters\n\nIncomplete data leads to flawed epidemiological response.\nDelayed contact tracing can plausibly translate into preventable infections and deaths.\n\n\n\n\nüìä‚ûó‚ö†Ô∏èüèõÔ∏è Reinhart & Rogoff economic growth error\n(Influential academic policy failure) Source: The Guardian, Retraction Watch\nA highly cited economic study claimed that countries with debt above 90% of GDP experienced sharply reduced growth. It was later discovered that an Excel formula excluded large parts of the data, significantly altering the result.\nLesson\n\nHidden formula errors can persist even in peer-reviewed research.\n\nWhy it matters\n\nThis work influenced austerity policies worldwide.\nSpreadsheet mistakes can shape economic policy affecting millions.\n\n\n\n\nüí∞üìë‚ùåüè¶ Fannie Mae misreported over $1 billion\n(Financial reporting failure) Source: The Guardian\nIn 2003, the U.S. mortgage giant reported quarterly results containing over $1 billion in accounting errors, traced back to an incorrect Excel formula.\nLesson\n\nFormula logic in spreadsheets is rarely audited or tested.\n\nWhy it matters\n\nFinancial misreporting undermines investor trust and regulatory oversight.\nSmall spreadsheet mistakes can become material financial risks.\n\n\n\n\nüêãüìâüí£üè¶ JPMorgan ‚ÄúLondon Whale‚Äù $6 billion loss\n(Risk modelling failure) Source: The Guardian\nA trading loss of approximately $6 billion was linked to risk models managed through manually edited Excel spreadsheets. Cut-and-paste workflows and poor version control amplified risk.\nLesson\n- Manual workflows lack audit trails and reproducibility.\nWhy it matters\n- Risk systems must be automated, tested, and transparent. - Spreadsheet-based models do not scale safely.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#spreadsheet-failures-in-biology-and-environmental-sciences",
    "href": "days/day01.html#spreadsheet-failures-in-biology-and-environmental-sciences",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Spreadsheet Failures in Biology and Environmental Sciences",
    "text": "Spreadsheet Failures in Biology and Environmental Sciences\nSpreadsheet misuse has caused systematic scientific errors, not just technical inconveniences. These are particularly relevant for biologists, where identifiers, codes, and metadata matter. The case study below is a more in depth examples in genomics. However, all the issues above are also applicable to many Environmental and Ecological data.\n\nüß¨üìÖ‚ùåüìä Gene name auto-conversion errors in genomics\nSpreadsheet software automatically converts certain character strings into dates or numbers. In genomics, this silently corrupts gene names such as SEPT2, MARCH1, or DEC1, which are converted into dates (e.g.¬†2-Sep, 1-Mar, 1-Dec) without warning.\nLarge-scale scans of published supplementary Excel files show that around 30% of papers containing gene lists are affected.\n\nPrevalence of Excel-induced gene name errors over time\n\n\n\nExcel-induced gene name errors over time\n\n\n\nSource: Abeysooriya et al.¬†(2021), PLoS Computational Biology\n\n\n\n‚ÄúThis [the error] is due to gene names being converted not just to dates and floating-point numbers, but also to internal date format (five-digit numbers). (Abeysooriya et al.¬†2021)‚Äù\n\nConcept that would have prevented this\n\nExplicit data typing (text ‚â† date ‚â† number)\nAvoiding spreadsheet auto-formatting for identifier columns\nUsing plain-text formats (CSV/TSV) with scripted imports",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#what-all-these-failures-have-in-common",
    "href": "days/day01.html#what-all-these-failures-have-in-common",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "What all these failures have in common",
    "text": "What all these failures have in common\nAcross intelligence, finance, economics, public health, and scientific research the same patterns recur:\n\nSilent data transformation\n\nHidden assumptions and formulas\n\nHard limits that are easy to exceed\n\nNo version control or audit trail\n\nTools used beyond their design scope\nOrganic design / Changing specs\nLack of testing\nLack of relevant knowledge and skills\n\nSpreadsheets did exactly what they were told.\nHumans misunderstood what they were doing.\nThis is why learning to use these tools properly matters. And this is why this course starts with Excel discipline, but does not end there.\n\nConcept that would have prevented this\n\nSeparation of raw data and derived results\nVersion control instead of manual file duplication\nExplicit metadata and data dictionaries\nWell designed data entry\nSkilled and knowledgeable users\nTreating spreadsheets as data entry interfaces and quick overview tools, not databases\nValidation rules and controlled vocabularies\nEarly transition to scripted, reproducible workflows\n\n\n\nCore data concepts reinforced by these cases\nAcross all examples, the same principles recur:\n\nData types must be explicit\nIdentifiers must never be auto-transformed\nRaw data must be immutable\nAnalysis must be reproducible (easier in script languages, but more on day 3)\nMetadata is part of the data\n\nThese principles motivate why this course starts with Excel discipline, but quickly moves to R and Quarto.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#excel-as-a-data-tool",
    "href": "days/day01.html#excel-as-a-data-tool",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Excel as a data tool",
    "text": "Excel as a data tool\nUsed correctly, Excel helps you:\n\nenter data consistently\ninspect data visually\ndetect obvious errors early\n\nUsed incorrectly, Excel:\n\nsilently changes data\nmixes metadata, logic, and values\nhides errors in formatting and formulas\n\nThe goal is discipline, not Excel mastery.\n\n\n\n\n\n\nImportant\n\n\n\nThere are alternatives to Excel (Google Sheets, LibreOffice Calc), but the principles remain the same.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#file-formats-metadata-and-storage",
    "href": "days/day01.html#file-formats-metadata-and-storage",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "File formats, metadata, and storage",
    "text": "File formats, metadata, and storage\n\nCommon Excel-related file formats\n\n\n\n\n\n\n\n\n\nFormat\nWhat it is\nWhen to use\nRisks\n\n\n\n\n.xlsx\nNative Excel workbook\nData entry, inspection\nProprietary, hides metadata\n\n\n.xls\nLegacy Excel\nAvoid\nObsolete, fragile\n\n\n.csv\nComma-separated values\nExchange, archiving\nNo metadata, no types, no formatting, no sheets\n\n\n.tsv\nTab-separated values\nExchange, robust to commas\nSame limits as CSV\n\n\n.txt\nPlain text with delimiter\nMaximum compatibility\nNeeds careful import\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey rule\n\nWork in .xlsx\nArchive and share in .csv / .tsv\n\n\n\n\n\nSeparators (delimiters): commas, tabs, semicolons, and why they matter\nPlain-text data files (.csv, .tsv, .txt) store tables using separators (also called delimiters) to mark where one column ends and the next begins.\nExcel does not store separators inside .xlsx files, but relies on them heavily when importing and exporting text data.\n\n\n\nCommon separators you will encounter\n\n\n\n\n\n\n\n\n\nSeparator\nFile extension\nTypical use\nCommon problems\n\n\n\n\nComma ,\n.csv\nDefault in many countries\nConflicts with decimal commas\n\n\nSemicolon ;\n.csv\nCommon in Europe\nMisread by software expecting commas\n\n\nTab \\t\n.tsv\nScientific data exchange\nInvisible, but robust\n\n\nPipe |\n.txt\nRare, custom exports\nRequires manual import\n\n\n\n\n\n\nWhy separators break things\nSeparators fail when they collide with content.\nExamples:\n\ncommas inside text fields (species names, notes)\ndecimal commas vs decimal points\ninconsistent exports across operating systems\nExcel guessing the wrong delimiter\n\n\nA file can be syntactically valid and still be interpreted incorrectly.\n\n\n\n\nCSV is not a standard (this surprises many people)\nDespite its name, CSV has no single global standard.\nCSV files vary by:\n\nseparator (, vs ;)\ndecimal symbol (. vs ,)\ntext encoding\nquoting rules\n\n\nWhat if the data itself contains a my separator?\nIf a value contains a separator, it must be wrapped in double quotes\nExample:\nplot_id, species, notes\n1, Iguana iguana, ‚Äúobserved near river, basking on rock‚Äù\n\n\n\n\n\n\nTry out creating a csv and a xlsx with comma and quotes as above (5 min)\n\n\n\nFrom Excel\n\nCreate a new sheet with the data as shown bellow\n\n\n\nSave as .csv and inspect the file in a text editor\n\nFile &gt; Save As &gt; ‚ÄúSave as type‚Äù &gt; CSV UTF-8 (Comma delimited) (*.csv)\nFrom text editor\n\nOpen a text editor (e.g.¬†Notepad, Notepad++, VSCode, TextEdit) What changed in the data? what is the delimiter?\nSave as .csv and open in Excel\nAfters inspecting, remove the last quote and re-open in Excel. What happens?\n\nNow repeat the process and add ; as well as , inside a value. Once you open the file in a text editor, what do you see?\nNow remove the quotes and re-open in Excel. What happens?\nNote How delimiter and quotes are handled depends on your data by excel.\n\n\n\n\nWhat if the data contains double quote\nIf a value contains a double quote ‚Äú, you escape it by doubling it.\nplot_id, species, notes\n1, Iguana iguana, ‚Äúobserved‚Äù‚Äúnear‚Äù‚Äù river, basking on rock‚Äù\n\n\n\n\nTSV is often safer than CSV\nTab-separated values (.tsv) are preferred in science because:\n\ntabs rarely appear in text\ncommas inside values do not break columns\nhuman-readable and machine-friendly\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you can choose: prefer TSV over CSV.\n\n\n\n\n\nHow Excel decides which separator to use\nExcel uses: - system locale settings - regional settings - guessing\nThis means:\n\nthe same file opens differently on different machines üòñ\nthe same file opens differently on Windows vs macOS ü§Æ\n\n\n\n\nText encodings\nText files do not store letters directly.\nThey store numbers that represent characters.\nA text encoding defines how those numbers map to letters, symbols, and accents.\nIf you feel motivated, check out this 10min video explaining encoding and related concepts\nCommon encodings you will encounter:\n\nUTF-8: modern standard, cross-platform, supports all languages\nWindows-1252 / Latin-1: older, limited, Windows-specific\nMacRoman: obsolete legacy format\n\n\n\n\n\n\n\nImportant\n\n\n\nBest practice\nAlways use UTF-8 for data files.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#encodings-are-invisible-until-they-break.",
    "href": "days/day01.html#encodings-are-invisible-until-they-break.",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Encodings are invisible until they break.",
    "text": "Encodings are invisible until they break.\nYou only notice encodings when something goes wrong:\n√§ becomes √É¬§\n¬µ becomes √Ç¬µ\nsymbols turn into ÔøΩ\ncolumn names become unreadable\nAt that point, the problem is already in the data, not just on the screen.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#hands-on-session-structured-data-entry",
    "href": "days/day01.html#hands-on-session-structured-data-entry",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Hands-on session: structured data entry",
    "text": "Hands-on session: structured data entry\n\n\n\n\nTraining datasets and templates\nAll practice files in data/raw and data/templates originate from the actual Belize protected-area monitoring programme plus the Sanchez-Martinez et al.¬†2025 drought-throughfall experiment and other linked datasets that will be downloaded directly. They have been lightly anonymized (IDs scrambled, locations jittered, sensitive observer metadata removed) to satisfy data privacy, permitting, and compliance rules while keeping the true structure we work with in the field. Because this site renders straight from GitHub, the following links open the exact files in the public repository.\nYou might want to download the entire repo, and acess the files locally for easier handling. Visit this, click on the green ‚Äú&lt; &gt; Code‚Äù button, and select ‚ÄúDownload ZIP‚Äù. After that unzip the file on your computer. \n\ndata/raw\n\ncsv/Caxiuana_tree_trait_data.csv ‚Äì canopy-throughfall trait table exported exactly as received from UKCEH, with only tree IDs hashed for privacy.\ncsv/Caxiuana_tree_trait_data_info.rtf ‚Äì companion metadata sheet (variables, methods, sensor notes) for the trait table.\nForesty/GAP_Data_2019 and Foresty/GAP_Data_2020 ‚Äì seasonal canopy-gap (GAP) monitoring exports for four permanent transects (files GAP_2014.05_* through GAP_2018.02_*) with sensitive stand IDs redacted.\nForesty/PLOT_Data_2019 and Foresty/PLOT_Data_2020 ‚Äì full-plot remeasurement workbooks for the same transects, cleaned only for ranger names and GPS precision.\nPSP/PSP_NPD42_OH.xlsx and PSP/PSP_NPD847_OH.xlsx ‚Äì Permanent Sample Plot (PSP) field exports with trunk/branch measurements, now using pseudonymous plot codes.\n\n\n\ndata/templates\n\nPSP/PSP_#####_SFBC.xlsx ‚Äì the clean data-entry template we fill during the field survey; replace ##### with your plot code.\nPSP/README.docx and PSP/[YYYYMM]_PSP_report.docx ‚Äì documentation that explains how rangers complete the template and assemble the monthly PSP report for supervisors.\nPSP/DW.png and PSP/DW2.png ‚Äì quick-reference visuals used in the field to remind teams of the DBH workflow and measurement checks.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#using-formatting-to-understand-data-not-decorate-it",
    "href": "days/day01.html#using-formatting-to-understand-data-not-decorate-it",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Using formatting to understand data (not decorate it)",
    "text": "Using formatting to understand data (not decorate it)\nFormatting is for error detection and data visualization, not aesthetics.\n\nMinimal but effective practices\n\n\n\n\n\n\nNote\n\n\n\n\nView ‚Üí Freeze Top Row (column headers; German Excel: Ansicht ‚Üí Fenster einfrieren ‚Üí Oberste Zeile einfrieren)\nView ‚Üí Freeze Panes (ID columns; German Excel: Ansicht ‚Üí Fenster einfrieren ‚Üí Fenster einfrieren)\nConditional formatting (Home ‚Üí Conditional Formatting; German Excel: Start ‚Üí Bedingte Formatierung) to:\n\nflag missing values\nhighlight out-of-range values\nreveal impossible measurements",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#defining-data-formats-explicitly-minimum-discipline",
    "href": "days/day01.html#defining-data-formats-explicitly-minimum-discipline",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Defining data formats explicitly (minimum discipline)",
    "text": "Defining data formats explicitly (minimum discipline)\n\n\n\n\n\n\nNote\n\n\n\nSteps\n\nSelect the column\nRight-click ‚Üí Format Cells (German Excel: Rechtsklick ‚Üí Zellen formatieren)\nSet:\n\nText for IDs\nNumber (fixed decimals) for measurements\nDate (explicit format) for dates\n\n\n\n\nThis prevents Excel from ‚Äúhelping‚Äù. Try this out in any of the files.\n\nExcel becomes a visual validator, not a calculator. Using those makes it harder to create and miss errors as well as for interpreting data.\n\n\n\n\n\n\nGroup exercise: Spot possible problems (10 min)\n\n\n\n\nDownload and check this supplementary material excel sheet/. This is a real scientific excel file used in a scientific study and is a hands on example on gene name errors in genomics.\nOpen the provided spreadsheets. You can double click it.. this data is already ‚Äúcorrupted‚Äù by excel.\nFreeze the top row.. see how it helps you navigate the data. (German Excel: Ansicht ‚Üí Fenster einfrieren ‚Üí Oberste Zeile einfrieren)\nSee line 466 and 467, row ‚ÄúgeneSymbol‚Äù. What looks suspicious? You can select the entire column and use conditional formatting to highlight values that are not text\n\nHome ‚Üí Conditional Formatting (German Excel: Start ‚Üí Bedingte Formatierung) ‚Üí Choose any Icon Set or Color Scale (obs. this will ignore text values)\n\nRight click the top and bottom cells and then Format Cells to see the current cathegory.\nSelect the entire column ‚ÄúgeneSymbol‚Äù properties and right click ‚Üí Format Cells ‚Üí Text. What happens to the values in that column?\nNow undo these two changes (Ctrl + Z or Cmd + Z).\nOn other cells at the same column, add possible geneSymbols like Sep3 and Sept21\nNow repete step 5. and then step 7.\n\nNote how did Excel auto-convert your new entries? What does this tell you about data entry in Excel?",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#common-data-formats-inside-excel-and-how-to-use-them",
    "href": "days/day01.html#common-data-formats-inside-excel-and-how-to-use-them",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Common data formats inside Excel (and how to use them)",
    "text": "Common data formats inside Excel (and how to use them)\nExcel cells always have two things: - a value - a format\nThe Formula Bar (located to the right of the Name Box) is also important; it typically displays the unformatted, underlying value or formula of the active cell, regardless of the format applied to the cell itself.\nConfusing these causes errors.\n\n\n\nFormat\nUse for\nCommon mistake\n\n\n\n\nText\nIDs, codes, species names\nAuto-conversion\n\n\nNumber\nCounts, measurements\nMixed units\n\n\nDate\nDates only\nIDs converted to dates\n\n\nTime\nTime of day\nMixed with dates\n\n\nLogical\nTRUE / FALSE\nStored as text\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRule\nIdentifiers are text, never numbers.\n\n\n\nCorrect import workflow (Excel 365)",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#importing-csv-tsv-and-delimited-text-correctly",
    "href": "days/day01.html#importing-csv-tsv-and-delimited-text-correctly",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Importing CSV, TSV, and delimited text correctly",
    "text": "Importing CSV, TSV, and delimited text correctly\n\nRefrain from double-clicking a CSV file. That is how Excel destroys data.\n\nThis prevents: - automatic date conversion - loss of leading zeros - unwanted scientific notation\n\n\n\n\n\n\nGroup exercise: Import CSV safely (10 min)\n\n\n\nSteps Power Editor\n\nData ‚Üí Get Data ‚Üí From File ‚Üí From Text/CSV (German Excel: Daten ‚Üí Daten abrufen ‚Üí Aus Datei ‚Üí Aus Text/CSV) Alternatively, if avaiable, you can use: Data ‚Üí From Text/CSV (German Excel: Daten ‚Üí Aus Text/CSV)\nSelect the file csv/Caxiuana_tree_trait_data.csv\nInspect:\n\ndelimiter (comma, tab, semicolon)\ndecimal separator\nencoding (UTF-8)\n\nClick Transform Data (German: Daten transformieren) to open Power Query Editor for more control. Use Load (German: Laden) only if you want to proceed with Excel default data transformation\n\nNote the type of the last variable (column) of line 32 from Texteditor or 33 from Power Query Editor. Note this is a number but in scientific format.\n\nExplicitly set column data types\n\nSelect columns\nRight-click ‚Üí Change Type (German Power Query: Datentyp √§ndern) ‚Üí choose correct type (Text, Decimal Number, Date, etc.) Alternatively you can change the type on the formula bar\n\nClick Close & Load (German: Schlie√üen & Laden) to import the data into Excel\n\nCheck the imported data. Did anything change?\nSteps Data Tools ‚Üí Text to Columns\nDo the same as above, but:\n\nOpening the .csv file from a text editor (e.g.¬†Notepad, Notepad++, VSCode, TextEdit) and copy-pasting the content into Excel.\nPaste it on the first cell (A1)\nSelect the entire first column (A)\nGo to Data ‚Üí Text to Columns (German Excel: Daten ‚Üí Text in Spalten)\nChoose Delimited (German: Getrennt) ‚Üí Next\nSelect different delimiters and see what happens with the dataset\nSelect the correct delimiter (Comma, Tab, Semicolon) as well as types ‚Üí Next (German: passenden Trenner w√§hlen, dann Weiter) What happens to the data? Check specifically for the last variable (column) of line 32. Right click it ‚Üí Format Cells and see the Cathegory and compare with the cell above it.",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#limiting-data-entry-to-avoid-mistakes",
    "href": "days/day01.html#limiting-data-entry-to-avoid-mistakes",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Limiting data entry (to avoid mistakes)",
    "text": "Limiting data entry (to avoid mistakes)\n\nData validation (essential)\n\n\n\n\n\n\nNote\n\n\n\nSteps\n\nSelect target cells or column\nData ‚Üí Data Validation (German Excel: Daten ‚Üí Daten√ºberpr√ºfung)\nChoose:\n\nnumeric range\nwhole numbers only\nallowed values\nno blanks\n\nAdd an input message if useful\n\n\n\nThis stops bad data before it enters the table.\n\n\n\nCreating drop-down lists (controlled vocabularies)\nUse drop-downs for: - habitat types - sex - life stage - presence / absence - categories\n\n\n\n\n\n\nNote\n\n\n\nSteps to create a drop-down list\n\nCreate a list of allowed values (preferably on a separate sheet)\nSelect the target cells\nData ‚Üí Data Validation (German: Daten ‚Üí Daten√ºberpr√ºfung)\nUnder Allow, choose List (German: Zulassen ‚Üí Liste)\nSet the source range (or type values manually)\nConfirm\n\n\n\nThis enforces consistency and avoids spelling variants.\n\n\n\n\n\n\n\nGroup exercise: Play with PSP data files (7 min)\n\n\n\n\nOpen any of the PSP xlsx files.\nIdentify at least three columns that need explicit formatting.\nApply the correct format to each column.\nUse formatting to check extreme values and missing data.\nSee data validation rules in action and spot possible improvements.\n\nBonus. Check PSP/PSP_NPD42_OH.xlsx and see if you can spot any data issues",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#avoiding-copy-paste-disasters",
    "href": "days/day01.html#avoiding-copy-paste-disasters",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Avoiding copy-paste disasters",
    "text": "Avoiding copy-paste disasters\nCopy-paste is one of the biggest sources of silent errors.\n\n\n\n\n\n\nWarning\n\n\n\nDefensive rules\n\nNever insert rows inside data blocks\nNever mix formulas and raw data\nKeep raw data formula-free\nUse Paste Special ‚Üí Values",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#sorting-and-ranking-data-safely",
    "href": "days/day01.html#sorting-and-ranking-data-safely",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Sorting and ranking data safely",
    "text": "Sorting and ranking data safely\nSorting a single column corrupts the dataset.\n\n\n\n\n\n\nNote\n\n\n\nSafe sorting workflow\n\nSelect the entire table\nData ‚Üí Sort\nConfirm Expand selection\n\n\n\n\n\n\n\n\n\nGroup exercise: try out bad and best practices with PSP data (7 min)\n\n\n\n\nOpen any of the PSP xlsx files.\nSelect a single column (e.g.¬†DBH) and sort it ascending. Home ‚Üí Sort & Filter ‚Üí Sort A to Z (German: Start ‚Üí Sortieren und Filtern ‚Üí Von A bis Z sortieren) or Right click ‚Üí Sort ‚Üí Sort Smallest to Largest (German: Rechtsklick ‚Üí Sortieren ‚Üí Kleinste bis Gr√∂√üte)\nObserve what happens to the rest of the data depending on your choice Continure with the current selectio would be catastrophic for the dataset! Values of variable DBH would be shuffled on the observations and no longer correspond to the rest of the data in the same row.\nNow move to the Analysis sheet and try addin a new row in the middle of a sequence of data\nObserve what happens to formulas in that sheet\nCopy a value and paste is with Ctrl+C and Ctrl+V in an empy cell. What happens?\n\nNow try pasting with Ctrl + Alt + V and select Values or Right click &gt; Paste Special &gt; Values (German: Rechtsklick ‚Üí Inhalte einf√ºgen ‚Üí Werte). What happens now?",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day01.html#freezing-structure-to-avoid-accidental-edits-and-improve-overview",
    "href": "days/day01.html#freezing-structure-to-avoid-accidental-edits-and-improve-overview",
    "title": "Day 1 ‚Äì What ‚Äògood‚Äô Data Is",
    "section": "Freezing structure to avoid accidental edits and improve overview",
    "text": "Freezing structure to avoid accidental edits and improve overview\n\n\n\n\n\n\nNote\n\n\n\n\nProtect sheets after data entry\nLock identifier columns\nAllow edits only where needed\n\n\n\nThis is basic but powerful.\n\n\n\n\n\n\nNote\n\n\n\nLocking structure (actual protection)\nExcel locks cells in two steps.\nStep 1: Unlock cells that may be edited\n\nSelect the cells that users are allowed to edit\nRight-click ‚Üí Format Cells\nGo to the Protection tab\nUncheck Locked\nClick OK\n\nStep 2: Protect the sheet\n\nGo to Review ‚Üí Protect Sheet\n(Optional) Set a password\nChoose allowed actions (e.g.¬†select unlocked cells only)\nConfirm\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll cells are locked by default,\nbut locking only takes effect after Protect Sheet is enabled.\n\n\n\n\n\n\n\n\n\nGroup exercise: Design a safe field data entry sheet (15min)\n\n\n\nWork in pairs.\nYou are designing a spreadsheet for PSP tree inventory data collected in the field.\nFor reference, use data/templates/PSP/PSP_#####_SFBC.xlsx\nYour table must include at least the following variables:\n\ntag_id (unique identifier, e.g.¬†metal tree tag)\nspecies_name (scientific name)\ndbh_cm (diameter at breast height, in cm)\nheight_m (tree height, in meters)\nquality_class (e.g.¬†good / damaged / dead)\n\n\nRequirements\nDesign the spreadsheet so that it is hard to enter bad data:\n\nfreeze the header row and identifier column\nexplicitly set data types for each column\nuse data validation:\n\nnumeric ranges for dbh and height\ndrop-down lists for quality_class and species names (you can just enter ABC‚Ä¶)\n\n\nüí¨ Be ready to explain - why each column has its format - what mistakes your design prevents\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nWhat surprised you today?\nWhat would you no longer trust Excel with?\nWhat surprised you about how errors occur?\nWhat would you like to know more about?\n\nIf you can, please take some time and give feedback on today‚Äôs session: - Anonymous feedback",
    "crumbs": [
      "Course Days",
      "Day 1 ‚Äì What 'good' Data Is"
    ]
  },
  {
    "objectID": "days/day03.html",
    "href": "days/day03.html",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\nexplain what an algorithm is and why formalizing steps is important\nread and write simple pseudocode to plan a solution\nunderstand core data types in R (numeric, character, logical, etc.) including how R handles missing values\nuse common data structures in R (vectors, matrices, data frames, lists) and understand their differences\nwrite basic R code with loops (for, while) and conditional statements (if)\ncreate and use your own functions in R to automate tasks\nimport and use packages in R, understanding the role of the R ecosystem\n\nNo prior programming experience is assumed, though familiarity with R basics from Day 2 will help.",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day03.html#learning-goals",
    "href": "days/day03.html#learning-goals",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "",
    "text": "By the end of this day, you should be able to:\n\nexplain what an algorithm is and why formalizing steps is important\nread and write simple pseudocode to plan a solution\nunderstand core data types in R (numeric, character, logical, etc.) including how R handles missing values\nuse common data structures in R (vectors, matrices, data frames, lists) and understand their differences\nwrite basic R code with loops (for, while) and conditional statements (if)\ncreate and use your own functions in R to automate tasks\nimport and use packages in R, understanding the role of the R ecosystem\n\nNo prior programming experience is assumed, though familiarity with R basics from Day 2 will help.",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day03.html#motivation",
    "href": "days/day03.html#motivation",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "Motivation",
    "text": "Motivation\nYesterday we saw that doing repetitive analysis manually is laborious. Programming allows us to express repetitive or complex tasks as a clear set of instructions ‚Äì an algorithm ‚Äì so the computer can execute them quickly and consistently. Just as a recipe guides a cook through steps to bake a cake, an algorithm guides the computer through steps to solve a problem. Today, we‚Äôll demystify programming by starting with simple logic and gradually building up to writing our own small functions in R. The goal isn‚Äôt to turn you into software engineers overnight, but to show how a little coding can save a lot of time and prevent errors.",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day03.html#agenda",
    "href": "days/day03.html#agenda",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "Agenda",
    "text": "Agenda\n13:30‚Äì14:15 | Block 1 ‚Äì Algorithms and pseudocode\n- Why we need algorithms: replacing repetitive manual work with clear instructions\n- Algorithms in everyday life (recipes, protocols)\n- Writing pseudocode: planning a solution in plain language\n- Flow of logic: sequence, decisions (if/else), repetition (loops)\n- Example: pseudocode for a simple data cleaning task\n14:15‚Äì14:30 | Break ‚òïÔ∏è\n14:30‚Äì15:15 | Block 2 ‚Äì Programming basics in R\n- Core data types in R: numeric, integer, character, logical; special values (NA, NaN, Inf, NULL)\n- Data structures: vectors (atomic vectors), matrices/arrays, lists, data frames (tibbles)\n- Indexing and subsetting data (extracting elements by position or name)\n- Vectorized operations in R (working with whole sets of values at once)\n- Loops (for loops to iterate; if statements for branching)\n- Comparing vectorization vs looping in R with examples\n15:15‚Äì15:30 | Break ‚òïÔ∏è\n15:30‚Äì16:15 | Block 3 ‚Äì Functions and tooling in R\n- Using built-in functions and understanding function arguments\n- Writing your own functions (syntax: function(name) { body })\n- Function examples: a simple conversion or calculation\n- Debugging basics: reading error messages, using print() for insight\n- The R ecosystem: packages and libraries (using install.packages and library to extend R‚Äôs capabilities)\n- Example: using a package (e.g., ggplot2 or readr) to illustrate how community-contributed tools enhance productivity\n- (Optional) Brief mention of RStudio features for coding (autocomplete, help pane)\n16:15‚Äì16:30 | Reflection, discussion, and outlook üß†\n- What concept was most challenging, and how might you get more practice?\n- Can you identify a repetitive task in your work that could be turned into an algorithm?\n- What would you like to automate or improve with programming going forward?\n- Preview of Day 4: cleaning data and ensuring reproducibility with code",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day03.html#from-manual-steps-to-algorithms",
    "href": "days/day03.html#from-manual-steps-to-algorithms",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "From manual steps to algorithms",
    "text": "From manual steps to algorithms\nUsing R effectively means learning to think like a programmer, which starts with understanding algorithms and logic.\nAlgorithm is a step-by-step procedure to accomplish a task. It‚Äôs like a cooking recipe: a clear sequence of instructions that leads to a desired outcome. In data analysis, an algorithm could be something like: ‚ÄúFor each observation, if the value is negative, mark it as an outlier; otherwise include it in the summary.‚Äù\nWriting out an algorithm forces you to be precise about what needs to be done ‚Äì and in what order. Even sorting a list of numbers or calculating a summary statistic involves an algorithm (either one you write, or one built into a software tool).\n\nWhat R is\n\nR is a tool to store things (numbers, text, tables) in objects, and then ask questions about them (summaries, plots, comparisons) or transform them.\n\n\n# The arrow means: store the value on the right into the name on the left\nx &lt;- 5\nx\n\n\n\nThinking in algorithms: a simple example\nLet‚Äôs say we want to compute the average (mean) of a list of numbers without using a built-in formula: 1. Take a list of numbers (our input). 2. Set a counter count to 0 and a sum to 0. 3. For each number in the list: add it to sum and add 1 to count. 4. After the loop, compute average = sum / count. 5. Output the average.\nThis is an algorithm for the mean. It‚Äôs conceptually what the formula does, but written as explicit steps. We could write it in plain English, as above, or in a structured\n\npseudocode:\nPseudocode: Calculate Mean\n\ninitialize sum = 0 \ninitialize count = 0 \nfor each value in the list: \n  sum = sum + value \n  count = count + 1 \nif count is 0: \n    return \"No data\" // avoid division by zero \nelse: \n    mean = sum / count return mean\n\nEven if you never coded before, this pseudocode is understandable. It‚Äôs essentially how you would explain the process to someone else. The computer, however, needs it in a more formal language (like R). But writing pseudocode first helps clarify your logic before worrying about R syntax.\n\n\n\n\n\n\nTip\n\n\n\nTip: When tackling a programming task, it often helps to write out the steps in pseudocode or plain language first. This separates the problem-solving from the syntax. Once the steps make sense, you can translate them into code.\n\n\n\n\nControl flow: sequence, selection, repetition\nAlgorithms often involve:\n\nSequence: executing instructions in order.\nSelection (branching): making decisions (if X is true, do this, otherwise do that).\nRepetition: doing something multiple times (looping).\n\nIn our mean calculation example:\n\nSequence: we had steps to follow one after another.\nSelection: we used an if to check if count is 0 at the end.\nRepetition: the ‚Äúfor each value‚Äù part is a loop.\n\nAnother everyday example ‚Äì a protocol for labeling samples:\nFor each collected sample: if sample has a barcode: scan the barcode else: assign a temporary ID and record it\nThis algorithm loops over each sample (repetition) and does one of two things depending on a condition (selection). Thinking algorithmically is largely about breaking problems into these logical components.",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day03.html#data-types-and-structures-in-r",
    "href": "days/day03.html#data-types-and-structures-in-r",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "Data types and structures in R",
    "text": "Data types and structures in R\nNow let‚Äôs bring these ideas into R. Programming in R (or any language) requires understanding how the language represents data and what operations it provides.\n\nAtomic data types in R\nR has a few atomic data types (the simplest building blocks), this :\n\nNumeric: for numbers (real numbers). Example: 42, 3.14. (R by default treats numbers as double-precision floats.)\n\n\nclass(10.1) #asks: How should R treat this conceptually?\n#response: \"numeric\"\ntypeof(10.1) #asks: How is this stored in memory?\n#response: \"double\"\n\n\nInteger: for integer numbers. Example: 42L (the L suffix denotes an integer in R, though usually R will convert to numeric if not specified).\nCharacter: for text strings. Example: \"Hello world\", \"Panthera onca\" (species name).\nLogical (Boolean): for TRUE/FALSE values. Example: TRUE, FALSE. (Often the result of comparisons like 5 &gt; 3 yields TRUE.)\nFactor: a special type for categorical data (underlyingly stored as integers with labels).\nComplex (for complex numbers) and Raw (for raw bytes) exist, but are rarely used in typical data analysis.\n\n\n\nüöÄüí•üí∂üí© European Space Agency‚Äôs Ariane 5 rocket failure\nwhy it matters to learn about data types\n\n\n\n\nAriane 5\n\n\n\nSource: ESA The European Space Agency\n\n\n\nLaunch date: 4 June 1996\nFailure time: 37 seconds after liftoff\nCost: ‚âà ‚Ç¨370 million\nCause: numeric overflow\nTrigger: conversion of a 64-bit floating-point value to a 16-bit signed integer\nWhy it happened: code reused from Ariane 4, where this value could never exceed the integer range\nResult: inertial reference system shut down, guidance lost, automatic self-destruction\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat ‚Äúdouble‚Äù actually means\nA double-precision number is stored as:\n\n1 bit: sign (positive or negative)\n11 bits: exponent (how big or small the number is)\n52 bits: mantissa (the significant digits)\n\nSo internally, a number looks like:\n¬± (significant digits) √ó 2^(exponent)\nBase 2, not base 10. That detail matters more than people expect. Because numbers are stored in binary, many decimal values cannot be represented exactly.\nExample:\n\n0.1 + 0.2 == 0.3\n\n[1] FALSE\n\n\nreturns FALSE in R.\nThis is not a bug, it‚Äôs binary arithmetic. What the computer actually stores is closer to:\n0.30000000000000004\nUsually harmless. Occasionally disastrous. Always worth remembering.\nReal world: climate and biodiversity models accumulating rounding error over thousands of iterations.\n\n\n\n\nThe blunt distinction (with ranges)\n\n\n\nAspect\nnumeric\ninteger\n\n\n\n\nStorage\ndouble precision float\n32-bit integer\n\n\nRange\napprox ¬±1.8 √ó 10¬≥‚Å∞‚Å∏\n‚àí2,147,483,648 to 2,147,483,647\n\n\nDecimal points\nallowed\nnot allowed\n\n\nExact representation\noften approximate\nexact (within range)\n\n\nDefault type\nyes\nno\n\n\nMemory\n8 bytes\n4 bytes\n\n\nClass\n\"numeric\"\n\"integer\"\n\n\n\n\n\nTwo clarifications worth remembering\nRange ‚â† precision\nNumerics can be astronomically large or tiny, but only about 15‚Äì16 decimal digits are reliable.\nIntegers are exact, but brutally finite.\nIntegers fail loudly, numerics fail quietly\nExceed the integer range and you get NA with a warning.\nPush doubles too far and you get rounding, overflow to Inf, or underflow to zero. No warning.\n\n#One-line R demo (integer overflow)**  \n.Machine$integer.max+0L\n#precision double\n1.00000000001-1\n1.00000000000001-1\n1.000000000000001-1\n1.0000000000000001-1\n\nR also has some special values:\n\nNA: ‚ÄúNot Available,‚Äù a missing value placeholder. There are NA versions for each type (numeric NA, character NA, etc., but usually just NA works and propagates appropriately).\nNaN: ‚ÄúNot a Number,‚Äù a result of undefined mathematical operations (like 0/0). It‚Äôs a type of numeric NA.\nInf and -Inf: positive or negative infinity (like the result of 1/0 or -1/0).\nNULL: represents the absence of any value or object (different from NA which is an element in a vector; NULL is often used to indicate ‚Äúnothing‚Äù as an object or list element).\n\nUnderstanding types matters because operations behave differently depending on type. For instance, adding two numbers is fine, but adding a number to a string results in an error. R will also sometimes coerce types in a vector (e.g., mixing numbers and text in the same vector will turn all into text).\n\n\nData structures in R\nBeyond single values, we have data structures to hold collections of values:\n\nVector: the most basic container, holding a sequence of values of the same type. A vector can be numeric, character, logical, etc. You can create one with c(), e.g.¬†weights &lt;- c(2.4, 5.1, 3.3) which is a numeric vector of length 3.\nMatrix: a 2D arrangement of values (like a table of one type, essentially a vector with dimensions). All entries in a matrix must also be of the same type. You can make a matrix with matrix() or by dimming a vector.\nArray: similar to a matrix but can have more than 2 dimensions (e.g., a 3D array).\nData frame: a table where each column can be a different type. This is like a spreadsheet or CSV file read into R ‚Äì the most common structure for datasets. Columns are vectors of equal length. For example, a data frame df might have one numeric column height, one character column species, etc. Each row represents an observation.\nList: a flexible container that can hold a collection of items of possibly different types and sizes. Lists are extremely useful in R because many complex objects (models, results) are stored as lists. You can think of a list as a general toolbox ‚Äì each element can be anything (a vector, a data frame, another list, ‚Ä¶). Create with list() function, e.g., my_list &lt;- list(name=\"Alice\", scores=c(90,95,88), info=data.frame(age=30, dept=\"Biology\")). Lists don‚Äôt require all elements to be same length or type.\n\nFor the Belize data, we‚Äôve mainly used data frames (for tables of observations). Under the hood, a data frame is actually a list of equal-length vectors (each column is a vector).\nBeing comfortable with vectors and data frames is key: - To access elements of a vector, use square brackets. weights[2] gives the second element of weights. - Data frames can be accessed with either df$column_name (to get a column as a vector) or df[row, column] indexing. For instance, df[5, \"species\"] would give the species of the 5th observation.\n\n\nExample: dealing with missing values\nSuppose we have a numeric vector of tree diameters and some are missing:\ndiameters &lt;- c(10.4, 8.1, NA, 9.5, 6.8)\nThe length(diameters) is 5, but one is NA. If we do mean(diameters), as noted, we get NA because by default R can‚Äôt compute the mean if any value is missing. We must do mean(diameters, na.rm = TRUE) to remove NAs in the calculation. We could also find which entries are missing by is.na(diameters) which returns a logical vector: FALSE FALSE TRUE FALSE FALSE in this case. We can use that to our advantage to filter or replace missing values. For instance, to filter out missing: clean_diameters &lt;- diameters[ ! is.na(diameters) ] Here ! is ‚Äúnot‚Äù, so !is.na(diameters) is TRUE for entries that are not NA, and we index the vector with that logical vector to keep only the non-missing values. This combination of vector operations and logical conditions is very powerful for data cleaning ‚Äì you can pick out elements meeting certain criteria without writing an explicit loop.\n\n\nVectorized thinking versus loops\nR is optimized to work with vectors. Many operations you might think of doing with a loop can be done in one go. For example, if we have two numeric vectors x and y of the same length, doing x + y will automatically add element by element (no loop needed in R code ‚Äì it loops internally in C, which is faster). This is vectorization. Similarly, if we have a vector of temperatures in Celsius and want to convert to Fahrenheit, we can simply do:\n\ntemps_c &lt;- c(20, 30, 25)\ntemps_f &lt;- temps_c * 9/5 + 32\n\nThis computes on each element of the vector automatically. We did not have to write\n\nfor (i in 1:length(temps_c)) { \n  temps_f[i] = temps_c[i]*9/5 + 32 \n} \n\n‚Ä¶. though we could do that and get the same result, it would be more verbose and usually slower.\nHowever, understanding loops is still important. Some tasks can‚Äôt be easily vectorized or it‚Äôs more intuitive to write as a loop first. Writing loops in R A for loop in R looks like: for (i in 1:5) { # code to run each time, using i } This would run the loop 5 times, with i taking values 1, 2, 3, 4, 5 in succession. We can loop over elements of a vector too:\n\nvalues &lt;- c(3, 7, 9)\nfor (v in values) {\n   print(v * 2)\n}\n\nThis would print 6, 14, 18. (Here v takes each value from values). We also have while loops which continue until a condition is false:\n\ncount &lt;- 1\nwhile (count &lt;= 5) {\n   print(count)\n   count &lt;- count + 1\n}\n\nThis will print 1 through 5. You have to be careful with while loops to ensure the condition eventually becomes false, otherwise you get an infinite loop.\nConditional execution The if statement in R:\n\nif (condition) {\n   # do something if condition is TRUE\n} else {\n   # do something if condition is FALSE\n}\n\nYou can also chain multiple conditions with else if:\n\nif (score &gt;= 90) {\n   grade &lt;- \"A\"\n} else if (score &gt;= 80) {\n   grade &lt;- \"B\"\n} else {\n   grade &lt;- \"C\"\n}\n\nR uses == for equality check (e.g., if (x == 0) {‚Ä¶}) and != for not equal. Also remember = is for assignment (though &lt;- is the preferred assignment operator in R), so use == for comparison. ! is used to negate a logical condition (e.g., if (!is.na(x)) {‚Ä¶}), meaning ‚Äúif x is not NA‚Äù. Before runing the bellow, try to think what will happen\n\n!c(TRUE, FALSE, NA)\n\n\n\n\n\n\n\n\nGroup exercise: Pseudocode to R code (10 min)\n\n\n\nConsider this task: You have a numeric vector scores of test scores. You want to count how many scores are above 80. 1. Write pseudocode for this task (just in plain steps or comments). 2. Translate your pseudocode into an R loop that accomplishes it. Try it on a sample vector scores &lt;- c(72, 88, 95, 60, 81).\n\n\nSolution\n\nPseudocode (one possible approach):\nset counter = 0\nfor each score in scores: if score &gt; 80: counter = counter + 1 return counter\nR code:\nscores &lt;- c(72, 88, 95, 60, 81)\ncounter &lt;- 0\nfor (s in scores) { if (s &gt; 80) { counter &lt;- counter + 1 } }\ncounter\ncounter should be 3 for the example vector (since 88, 95, 81 are &gt; 80)\nAlternatively, a vectorized solution without an explicit loop would be:\nsum(scores &gt; 80)\nsince scores &gt; 80 produces a logical vector and sum() of TRUE/FALSE treats TRUE as 1 and FALSE as 0.\n\n\n\n\nIn the solution above, we also showed a more idiomatic R way: using sum(scores &gt; 80) to count, or even table(scores&gt;80). This highlights how R‚Äôs vector operations can simplify tasks. But it‚Äôs still important to learn loops and if logic, because not everything can be neatly vectorized, and clarity sometimes trumps cleverness.\n\n\nUnderstanding functions in R\nIn Day 2, we freely used functions like mean() or read.csv(). Functions are like mini-programs: they take inputs (arguments) and return an output. In R, you can write your own functions to avoid repeating yourself.\nFor example, suppose throughout our analysis we need to convert lengths from centimeters to inches (maybe some collaborator uses inches). Instead of repeatedly writing value/2.54 (since 1 inch = 2.54 cm), we can write a function:\n\ncm_to_in &lt;- function(cm) {\n  inches &lt;- cm / 2.54\n  return(inches)\n}\n\nNow we have defined cm_to_in() that we can call anywhere: cm_to_in(10) # returns 3.937008 inches\nIf we pass a vector, it will return a vector (because the division operation is vectorized).\nFunctions can have multiple arguments, default values, etc. The general form is: my_function &lt;- function(arg1, arg2 = default_value, ‚Ä¶) { # code that uses arg1, arg2, ‚Ä¶ # optionally a return() call (if omitted, the last expression‚Äôs value is returned) }\nWhy write functions? - Avoid repetition: If you find yourself copying and pasting a block of code with minor changes, that‚Äôs a candidate for a function. Write it once, parametrize the parts that change (as arguments), then call it whenever needed. - Clarity: Good function names make code easier to read (‚Äúcalculate_index(df)‚Äù is more descriptive than a dozen lines of code doing it in place). - Fix it once: If you need to change the logic, you only update the function, not every occurrence in your code. - Testing: You can test a function on known inputs (unit testing) to be confident it works, then use it throughout your analysis.\n\n\n\n\n\n\nGroup exercise: Write a simple function (5 min)\n\n\n\nWrite an R function fahrenheit_to_celsius(temp_f) that converts a temperature from Fahrenheit to Celsius. The formula is \\(C = (F - 32) \\times 5/9\\). Test your function on the values 32¬∞F (should get 0¬∞C) and 98.6¬∞F (should get ~37¬∞C).\n\n\nSolution\n\nFunction definition: fahrenheit_to_celsius &lt;- function(temp_f) { temp_c &lt;- (temp_f - 32) * 5/9 return(temp_c) } Testing it: fahrenheit_to_celsius(32) # returns 0 fahrenheit_to_celsius(98.6) # returns 37 (actually 37 exactly in this case) Notice that if you pass a vector, e.g.¬†fahrenheit_to_celsius(c(32, 212)), it will return 0 100 (since 212¬∞F is 100¬∞C). The function operates on each element, leveraging R‚Äôs vectorized arithmetic.\n\n\n\n\n\nA brief note on packages\nR‚Äôs power comes not only from the base language, but from its vast ecosystem of packages. A package is a collection of functions, data, and documentation that extends R‚Äôs capabilities. For example: - dplyr: for data manipulation (we‚Äôll use it later for cleaning). - ggplot2: for advanced plotting. - readr: faster CSV reading functions. - sf: for spatial data operations. - ‚Ä¶ and thousands more for every niche (genomics, image analysis, etc.). To use a package, you first install it (one-time) with install.packages(‚ÄúpackageName‚Äù), then load it in each session with library(packageName). Once loaded, you can use the functions it provides. For instance: install.packages(‚Äúggplot2‚Äù) # run once library(ggplot2) # load the package ggplot(data = trees, aes(x = species, y = dbh_cm)) + geom_boxplot() This would create a nicer boxplot of dbh_cm by species using the ggplot2 grammar of graphics. We won‚Äôt dive deep into packages today, but it‚Äôs good to know they exist. In fact, R itself is a collection of packages (base R plus recommended). When you load RStudio, some packages like stats and utils load by default. R philosophy: There‚Äôs a saying, ‚ÄúThere‚Äôs probably a package for that.‚Äù If you find yourself needing functionality beyond basic R, chances are someone in the community has created a package. Reusing these is often better than reinventing the wheel. It‚Äôs a key skill to learn how to find and use packages (CRAN, Bioconductor, GitHub are common sources) as you progress.\n\n\n\n\n\n\nClassic values in programming (and why they exist)\n\n\n\n\nüî¢ Correctness over cleverness\nCode must first be right. Elegant nonsense is still nonsense.\nMost catastrophic bugs are not complex, they are wrong.\nüßÆ Explicit types matter\nKnowing whether something is an integer or a floating-point number is not pedantry.\nIt is the difference between counting trees or measuring height.\n‚ö†Ô∏è Limits are real\nEvery data type has bounds. Ignore them and reality will remind you. Loudly.\nFamous example: a rocket self-destructed because a 64-bit float was forced into a 16-bit integer. Overflow. Boom.\nüß† Clarity beats brevity\nCode is read more often than it is written.\nYour future self is a hostile reviewer with no patience.\nüîÅ Reproducibility is a feature, not a luxury\nIf you cannot rerun it, you did not really do it.\nScience and software share this moral spine.\nüß™ Fail fast, fail loud\nSilent errors are the most dangerous kind.\nWarnings and crashes are annoying. Wrong results are lethal.\nüìè Precision is not accuracy\nMore digits do not mean more truth.\nDouble precision gives you range and speed, not philosophical certainty.\nüöÄ Never assume ‚Äúthis will never happen‚Äù\nIt already has.\nUsually at scale.\nUsually in production.\nSometimes at launch.",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day03.html#wrap-up",
    "href": "days/day03.html#wrap-up",
    "title": "Day 3 ‚Äì From repetition to algorithms",
    "section": "Wrap-up",
    "text": "Wrap-up\nToday we bridged the gap between knowing what we want to do and telling the computer how to do it. We discussed algorithms as the logical recipes behind every analysis task, and we practiced writing these instructions in R. We covered a lot of ground: R data types and structures, how to control the flow of a program with loops and conditions, and even how to write your own functions to extend R. You might be thinking, ‚ÄúThat‚Äôs a lot to remember!‚Äù And indeed, becoming comfortable with programming takes practice. The key takeaway is that programming is about breaking problems into clear steps and knowing the tools R provides to implement those steps. With these fundamentals, you can start to tackle tasks that were tedious or impossible to do in Excel, like looping through hundreds of files or applying complex logic consistently. Our Belize case study might involve, for example, writing a loop to go through many survey files and extract a summary from each ‚Äì something that would be extremely painful to do by hand. With your new skills, such tasks are within reach. In the next session (Day 4), we will apply programming to clean and transform data, pushing further into making our workflow reproducible. We‚Äôll see how writing scripts and using version control can make our scientific work more reliable and shareable. Don‚Äôt worry if not everything clicked immediately. You now have a mental model of what‚Äôs possible, and each time you practice, these concepts will become more natural. Use the exercises and examples as references. And remember: even experienced programmers frequently look up documentation and examples ‚Äì that‚Äôs part of the process!\n\n\n\n\n\n\n\nReflection\n\n\n\n\nWhich part of programming today was most challenging to you (e.g., understanding a loop vs.¬†vectorized approach, or remembering the syntax)?\nThink of a repetitive task in your own work or research. How could you break it into algorithmic steps, and what benefit would automating it bring?\nAre there any operations or analyses you wish you could do faster or more reliably? Keep those in mind as motivation to practice and learn more coding. Please provide your feedback on today‚Äôs session: Anonymous feedback",
    "crumbs": [
      "Course Days",
      "Day 3 ‚Äì From repetition to algorithms"
    ]
  },
  {
    "objectID": "days/day05.html",
    "href": "days/day05.html",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "",
    "text": "By the end of this final day, you should be able to:\n\nexplain the role of models in science as simplified representations of reality, and identify examples of models in biology\nrecognize the strengths and limitations of different tools (Excel, R, others) for various stages of the data lifecycle\ncritically assess how tool choice can influence scientific results (e.g., potential biases or errors introduced by tools)\narticulate the principles of responsible data analysis (ethical use of data, transparency, acknowledging uncertainty)\nreflect on your personal learning over the course and identify next steps to continue improving your data skills\nparticipate in an informed discussion about digital tools and methods in science, providing constructive feedback and insight",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#learning-goals",
    "href": "days/day05.html#learning-goals",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "",
    "text": "By the end of this final day, you should be able to:\n\nexplain the role of models in science as simplified representations of reality, and identify examples of models in biology\nrecognize the strengths and limitations of different tools (Excel, R, others) for various stages of the data lifecycle\ncritically assess how tool choice can influence scientific results (e.g., potential biases or errors introduced by tools)\narticulate the principles of responsible data analysis (ethical use of data, transparency, acknowledging uncertainty)\nreflect on your personal learning over the course and identify next steps to continue improving your data skills\nparticipate in an informed discussion about digital tools and methods in science, providing constructive feedback and insight",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#agenda",
    "href": "days/day05.html#agenda",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "Agenda",
    "text": "Agenda\n13:30‚Äì14:15 | Block 1 ‚Äì Models: simplified representations of reality\n- What is a model? (conceptual, mathematical, statistical models)\n- Examples of models in biology (population growth equations, climate models, statistical models like linear regression)\n- ‚ÄúAll models are wrong, but some are useful‚Äù (George Box) ‚Äì understanding the quote in context[2]\n- The balance between simplicity and complexity: why we model and how to judge a model‚Äôs usefulness\n- Limitations of models: what they leave out, assumptions made (connecting back to Day 1 where data itself is a simplified model of reality)\n- How computational tools (Excel, R) implement models (e.g., Excel formulas vs R functions vs specialized software)\n14:15‚Äì14:30 | Break ‚òïÔ∏è\n14:30‚Äì15:15 | Block 2 ‚Äì Tool choice as a scientific decision\n- Recap of the data lifecycle and where different tools shine or falter (Excel in early stages vs R in analysis, etc.)\n- Strengths & weaknesses of Excel: intuitive, great for entry and simple calculations, but prone to human error, not scalable, not reproducible\n- Strengths & weaknesses of R: powerful, reproducible, handles large data, steep learning curve, requires coding\n- Other tools in the ecosystem: databases (SQL) for storage, Python or others for specific tasks, domain-specific software (GIS for spatial data, etc.)\n- Deciding factors: data size, complexity of analysis, need for collaboration, future reuse ‚Äì how these influence tool selection\n- Avoiding tool fanaticism: the goal is correct and credible science, not proving one tool is superior ‚Äì often a combination is best (e.g., use Excel for field data entry templates, R for analysis)\n- Real-world scenario discussion: Given a project (e.g., biodiversity survey with multiple collaborators), what tools/workflow would you choose and why?\n15:15‚Äì15:30 | Break ‚òïÔ∏è\n15:30‚Äì16:15 | Block 3 ‚Äì Scientific responsibility and course wrap-up\n- Data ethics and responsibility: handling data honestly (no cherry-picking or p-hacking), respecting privacy when applicable, and giving proper attribution\n- The importance of transparency: sharing data and code (when possible) for reproducibility ‚Äì how this builds trust in your findings\n- Reproducibility vs.¬†replication: our responsibility to make analyses reproducible, and the role of the broader scientific community in verifying results\n- Personal responsibility: knowing your tools‚Äô limits (e.g., Excel‚Äôs floating point errors, R‚Äôs package reliability), and validating results (cross-checking critical calculations by hand or with alternative methods)\n- Course reflection: what have we learned across the 5 days? Open discussion of key takeaways, surprises, and remaining questions\n- Feedback session (official course evaluation)\n- Resources for continued learning: pointing to tutorials, communities (like RStudio Community, Stack Overflow, etc.), and recommending practice through real projects\n- Closing remarks and discussion: building a culture of data competence and confidence in your labs/teams",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#models-capturing-the-essence-of-reality",
    "href": "days/day05.html#models-capturing-the-essence-of-reality",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "Models: capturing the essence of reality",
    "text": "Models: capturing the essence of reality\nIn Day 1 we said that data are a simplified representation of reality. Models take this a step further: they are deliberate constructions (often mathematical or computational) that approximate a real-world process. A model could be: - Conceptual: e.g., the food chain model in an ecosystem (who eats whom), or a flowchart of a biological pathway. - Physical: e.g., a scale model of an ecosystem in a terrarium. - Mathematical/Statistical: e.g., a formula or algorithm. The logistic growth equation for population size is a model of how populations grow with a carrying capacity. A linear regression is a model relating a response to predictor(s).\nWhy use models? Because reality is complex. Models let us focus on key features and make predictions or gain insight. For example, to forecast fish population in Belize rivers, we might use a model incorporating birth/death rates and environmental factors. The model won‚Äôt capture every nuance (poaching, disease outbreaks, etc., might not be in the model), but it gives a rough expectation.\nGeorge Box famously said, ‚ÄúEssentially, all models are wrong, but some are useful.‚Äù This reminds us that: - No model is a perfect mirror of reality (if it were, it would be reality, which is impossible). - A model‚Äôs value is judged by its usefulness for a purpose. Does it predict accurately within tolerable error? Does it help us understand the dominating factors?\n\nExamples of models in biology\n\nPopulation dynamics: models like exponential or logistic growth, Lotka-Volterra predator-prey equations.\nClimate models: highly complex models that simulate climate systems to predict changes ‚Äì clearly approximate, yet vital for scenario planning.\nStatistical models: a linear model relating, say, tree growth to rainfall and soil type. It‚Äôs ‚Äúwrong‚Äù in the sense it‚Äôs a simplification (maybe many other factors at play), but it can be useful to quantify relationships and make predictions.\nMachine learning models: these can be seen as black-box models predicting outcomes (like species identification from photos). They are powerful but require careful validation to ensure they generalize.\n\n\n\nUsing tools to implement models\nTools like Excel and R allow us to build and apply models: - In Excel, you might construct a model with formulas across cells (like a simulation model iterating in rows), or use the Solver add-in for optimization models. Excel is transparent for simple models but becomes cumbersome for complex ones. - In R, you can write programs to simulate models (loops, differential equation solvers) or fit statistical models with built-in functions (lm() for linear models, etc.). R excels at this because of packages: want to model population genetics? There‚Äôs likely a package for that. Want to do a regression? One command and you get coefficients and diagnostics. - Other specialized tools: Python (with libraries), MATLAB for certain simulations, or software like Stella for system dynamics ‚Äì the choice depends on the field and complexity.\nThe key is to remember the assumptions behind any model you use. For example, a linear regression assumes a linear relationship and certain error distribution. If those don‚Äôt hold, the model may give misleading results.\nAs scientists, we must: 1. Choose appropriate models for our questions. 2. Validate models with data (e.g., compare model predictions to held-out observations). 3. Communicate uncertainty ‚Äì confidence intervals, error bars, scenario ranges.\nUsing models responsibly ties into the next topics: choosing appropriate tools and being transparent.",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#choosing-the-right-tool-for-the-job",
    "href": "days/day05.html#choosing-the-right-tool-for-the-job",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "Choosing the right tool for the job",
    "text": "Choosing the right tool for the job\nThroughout this course, we juxtaposed Excel and R as tools. Let‚Äôs summarize their roles across the data lifecycle (planning, collection, cleaning, analysis, visualization, sharing):\n\nExcel (or spreadsheets):\n\nStrengths: Excellent for data entry (familiar interface for humans), quick one-off calculations, exploratory pivoting of small datasets, making simple charts for immediate visualization. It‚Äôs ubiquitous ‚Äì collaborators likely have it ‚Äì and it requires minimal training for basics.\nWeaknesses: Doesn‚Äôt scale well to large data (millions of rows), prone to human errors (accidental sorting, overwriting formulas, etc.), lacks reproducibility (hard to track exact steps taken), and has limited statistical modeling capabilities. Also, as we saw, it might auto-misformat data (e.g., gene names to dates).\nBest use: Early data recording, lightweight analysis, data inspection, or as a reporting tool after analysis (some people use Excel to present results or allow others to interact with summary tables).\n\nR (and programming in general):\n\nStrengths: Handles large and complex data, offers advanced analysis and modeling libraries, ensures reproducibility through scripts, and integrates into automated pipelines. Great for statistics, simulations, and creating publication-quality graphics (especially with packages like ggplot2).\nWeaknesses: Steeper learning curve ‚Äì requires knowledge of code and the quirks of R. Not ideal for raw data entry (you wouldn‚Äôt have a field technician type into an R console!). Sometimes setting up the environment (installing packages, dealing with version differences) can be a hassle. For someone doing a very simple task infrequently, writing code might overhead compared to a quick Excel edit (but that flips if the task repeats or needs rigor).\nBest use: Data cleaning, merging, complex transformations, statistical analysis, producing reproducible reports, handling iterative or repeatable analyses, and basically anything beyond trivial data tasks.\n\nSQL databases (not covered in depth here):\n\nGreat for storing and querying very large datasets that exceed what Excel or even R in memory can handle. For instance, a nation-wide biodiversity database might live in PostgreSQL or similar, and you use SQL queries (potentially from R) to pull subsets for analysis.\nIf your project grows, learning basic SQL could be a next step to manage data.\n\nPython:\n\nOften mentioned alongside R. Python, with libraries like pandas, is also a powerful data tool, more general-purpose than R (used in web dev, etc., so it‚Äôs multi-domain). For data tasks, Python vs R often comes down to preference and specific libraries available. R is traditionally stronger in statistics and reporting, Python in machine learning and integration with other software. Knowing one makes learning the other easier if needed.\n\nSpecialized tools:\n\nGIS software (ArcGIS/QGIS) for spatial data ‚Äì e.g., mapping habitats in Belize, doing spatial analysis (R can do a lot of GIS too with sf and raster packages, but dedicated GIS software offers interactive mapping).\nImage analysis software (ImageJ, etc.) for biological imaging.\nMATLAB/Octave often used in bioengineering or for certain simulations.\nCloud platforms for big data or collaboration (Google Sheets for simple collaborative data entry, cloud computing for heavy analysis).\n\n\nThe overarching point: tool choice is part of experimental design. Just as you choose an appropriate instrument to measure a phenomenon, you choose appropriate software tools to handle data. A poor tool choice can introduce errors or inefficiencies: - Example: Using Excel to manually curate data coming from an automated sensor every day could lead to copy-paste mistakes and burnout ‚Äì better to set up a script. - Conversely, insisting on using R for a small dataset that one colleague only has in Excel could create an unnecessary barrier ‚Äì maybe the compromise is you do the heavy analysis in R but export a user-friendly Excel summary for colleagues.\nThere‚Äôs also the dimension of community and support: if everyone in your lab uses R, it‚Äôs beneficial to join that for shared knowledge. If everyone uses something else, maybe you become the catalyst introducing R ‚Äì but expect a transition period.\n\n\n\n\n\n\nNote\n\n\n\nDiscussion: We‚Äôll discuss specific scenarios (e.g., a conservation NGO needs to analyze quarterly field reports) and debate which toolset fits best. Often, a hybrid approach wins: e.g., use Excel templates for data collection, R scripts for analysis, and perhaps an Excel or web dashboard for presenting results to stakeholders.\n\n\nTo emphasize, our goal is not to declare ‚ÄúR good, Excel bad‚Äù ‚Äì both have a place. It‚Äôs about using each for what it‚Äôs good for, and being aware of limitations. Through this course, you‚Äôve seen where errors can creep in (Excel autofill issues, etc.) and how code can mitigate some (but code has its own pitfalls like bugs, which version control and testing help catch).\nFinally, keep in mind the cost/benefit: investing time to learn a more advanced tool like R has upfront cost but long-term benefit if your work continuously involves data. For a one-time small analysis, maybe a quick spreadsheet is fine. For a research career with lots of data, learning R (and/or Python, SQL, etc.) is immensely beneficial ‚Äì it‚Äôs an investment in efficiency and capability.",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#with-great-power-comes-great-responsibility",
    "href": "days/day05.html#with-great-power-comes-great-responsibility",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "With great power comes great responsibility",
    "text": "With great power comes great responsibility\nUsing powerful tools (and models) in science gives us great capability ‚Äì but also the responsibility to use them correctly and ethically.\n\nEthical data practice\n\nHonesty and transparency: Never manipulate data or tweak a model just to get a desired outcome. It sounds obvious, but subtle temptations exist ‚Äì like excluding ‚Äúoutliers‚Äù that inconveniently disprove your hypothesis without a valid reason. Always document why you exclude or alter data points. Tools like R make it easier to show exactly what was done.\nAvoiding p-hacking: If you try 100 different analyses and only report the one that gave a significant result, that‚Äôs not good science. Be upfront about how you chose your analysis approach. Pre-registering studies or at least stating ‚Äúthis analysis is exploratory‚Äù vs ‚Äúconfirmatory‚Äù is good practice.\nData privacy: If you work with sensitive data (e.g., human subjects, or locations of endangered species that poachers could misuse), be responsible in how you store and share data. This might mean anonymizing data or aggregating to protect identities/locations.\nReproducibility: Aim to make your analysis such that others can reproduce it. This has been a theme all week. It‚Äôs part of being responsible ‚Äì it allows science to be self-correcting. If an error is found, it can be traced and fixed; if results are solid, they‚Äôll be confirmed.\nCredit and attribution: Use tools and data with proper citation. If you use an R package, cite it in publications. If a colleague gave you an Excel sheet of data, acknowledge them. Science is a community effort.\n\n\n\nKnowing the limits\nRemember the limitations of both your data and your tools: - If your model makes a lot of assumptions (like a normal distribution, or no collinearity in regression predictors), check those assumptions. Using R, you can inspect diagnostic plots or perform tests. Using Excel, you might not even be aware of assumptions if you just click a trendline ‚Äì so tools like R can force you to think more deeply. - If you push Excel beyond its reliable limits (like very large datasets), know that you might get sluggish performance or crashes ‚Äì or silent truncation of data (Excel has row limits). Responsible practice is to choose a more suitable tool or break data into pieces. - R can handle big data, but memory is finite ‚Äì and writing extremely inefficient code can stall things. A responsible analyst monitors their workflow (maybe using sample data first, or adding checkpoints) to not get stuck.\n\n\nReflection on the course journey\nOn Day 1, we confronted the idea that data and tools can fail us if used naively (remember the corrupted gene names example, or poorly structured spreadsheets). Across Day 2‚Äì4, we built up a toolkit and mindset: - Day 2: Summarizing and visualizing data ‚Äì learning to extract meaning and be wary of misleading visuals or stats without context. - Day 3: Programming and algorithms ‚Äì realizing that explicitly telling the computer what to do (and how) opens up possibilities to automate and scale analyses. - Day 4: Cleaning and reproducibility ‚Äì appreciating that a lot of work happens before the ‚Äúanalysis‚Äù proper, and that documenting and structuring this work is part of being a scientist in the digital age. - Day 5 (today): Stepping back to see the big picture of how we wield these tools and techniques responsibly to do good science.\nConsider how your perspective has changed. Maybe Excel‚Äôs convenience is now balanced in your mind with caution about its pitfalls. Maybe R went from something intimidating to something you‚Äôve actually written code in. Perhaps the importance of documenting steps (in code or writing) is more clear now.\n\n\nContinuing the journey\nThis course is a beginning, not an end. Data competence is like a muscle ‚Äì you have to exercise it: - Continue using what you learned: even if small, try writing an R script for a task, or using Quarto for a report. Don‚Äôt worry if you need to look up things ‚Äì that‚Äôs normal (we all Google function syntax or error messages). - Engage with the community: there are forums (Stack Overflow‚Äôs R section, RStudio Community, local R user groups) where you can ask questions and see how others solve problems. Often, someone has faced the same challenge. - Learn incrementally: you might dive deeper into a specific area next ‚Äì e.g., if you deal with spatial data, learn the sf package; if stats, maybe a course or book on applied statistics with R; if data visualization, explore advanced ggplot2 or interactive viz with Shiny. - Encourage a culture of data responsibility: If you work in a team, share tips or set up small workflows inspired by this course (for example, decide that all data will have a README and version control, or start peer-reviewing each other‚Äôs analysis scripts). It might be novel in some settings, but it often only takes one person to spark a positive change.\nFinally, maintain a critical eye and curiosity. Tools will evolve (maybe in 10 years we‚Äôll use something beyond R or Excel). But the core principles ‚Äì clear thinking about data, honesty, and reproducibility ‚Äì are enduring.",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#overview-of-the-data-lifecycle-and-where-excel-and-r-fit",
    "href": "days/day05.html#overview-of-the-data-lifecycle-and-where-excel-and-r-fit",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "Overview of the data lifecycle (and where Excel and R fit)",
    "text": "Overview of the data lifecycle (and where Excel and R fit)\nThink of data not as a file, but as a process with phases. Most data disasters happen when a tool designed for one phase is silently used for all phases.\nBelow, each stage of the data lifecycle is explained once. Later, Excel and R are positioned relative to these stages.\n\n\n1) Data generation\nWhat this stage is about\nData are created in the real world: - measurements - observations - experiments - surveys - sensors - field notes\nBiology examples - species counts - trait measurements - GPS locations - lab assays - environmental monitoring\nGood tools - field sheets - instruments - sensors - lab notebooks - electronic lab notebooks (ELNs)\nExcel - ‚ùå Not involved\nExcel does not generate data. It can only record values after they exist.\nR - ‚ùå Not involved\nR does not generate empirical data, except in simulations.\n\n\n\n2) Data entry and initial structuring\nWhat this stage is about - transcribing observations into digital form - defining rows, columns, units, identifiers - first human sanity checks\nGood tools - Excel - Google Sheets - LibreOffice Calc - simple data entry forms\nExcel - ‚úÖ Excellent here (when used carefully)\nThis is Excel‚Äôs natural habitat: - fast manual entry - immediate visual feedback - simple validation rules - human-readable tables\nR - ‚ö†Ô∏è Not designed for this\nR expects data to already exist in files or databases.\nKey rule - one row = one observation\n- one column = one variable\n- no analysis logic in raw data\n\n\n\n3) Data cleaning and quality control\nWhat this stage is about - fixing typos - harmonizing codes - handling missing values - checking ranges and consistency\nGood tools - Excel (small datasets, transparent fixes) - R - Python (pandas)\nExcel - ‚ö†Ô∏è Limited and risky\nExcel can clean data, but: - changes are hard to audit - copy-paste errors creep in - history is lost\nR - ‚úÖ Very strong\nCleaning steps are scripted, explicit, and reproducible.\nKey transition point &gt; If you cannot explain exactly what you changed and why, you should move out of Excel.\n\n\n\n4) Data storage and archiving\nWhat this stage is about - long-term storage - sharing with collaborators - submission to repositories\nGood tools - CSV / TSV - databases - repositories (Zenodo, Dryad, institutional servers)\nExcel - ‚ùå Poor choice\nExcel files: - hide metadata - embed formatting and formulas - are fragile across versions - are not diff-friendly\nExcel is not a database.\nR - ‚ö†Ô∏è Indirect\nR produces clean, portable data files, but is not itself a storage system.\n\n\n\n5) Data analysis\nWhat this stage is about - statistical summaries - models - hypothesis testing - simulations\nGood tools - R - Python - specialized statistical software\nExcel - ‚ùå Danger zone\nExcel allows analysis, but: - formulas are opaque - errors are hard to detect - results are not reproducible - reviewers cannot re-run your work\nThis is where many famous failures occurred.\nR - ‚úÖ Core strength\nTransparent, testable, and reproducible analysis.\n\n\n\n6) Visualization and communication\nWhat this stage is about - figures - tables - reports - presentations\nGood tools - R (ggplot2) - Python (matplotlib, seaborn) - Quarto - Illustrator (final polishing)\nExcel - ‚ö†Ô∏è Quick look only\nExcel is fine for: - exploratory plots - sanity checks\nNot for final, reproducible figures.\nR - ‚úÖ Excellent\nPublication-quality plots with full control and consistency.\n\n\n\n7) Reproducibility and reuse\nWhat this stage is about - others reusing your data - you revisiting it months later - reviewers asking ‚Äúhow did you get this?‚Äù\nGood tools - scripts - version control (Git) - Quarto / notebooks\nExcel - ‚ùå Actively hostile\nExcel workflows are: - undocumented - non-repeatable - person-specific\nR - ‚úÖ Designed for this\nScripts record every step and can be re-run exactly.",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#core-message",
    "href": "days/day05.html#core-message",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "Core message",
    "text": "Core message\nExcel is a front-end tool for humans.\nR is a logic engine for trust.\n\nUse Excel to enter and see data.\nUse R to clean, analyse, and justify results.",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "days/day05.html#excel-vs-r-across-the-data-lifecycle-detailed-comparison",
    "href": "days/day05.html#excel-vs-r-across-the-data-lifecycle-detailed-comparison",
    "title": "Day 5 ‚Äì Tools, models, and scientific responsibility",
    "section": "Excel vs R across the data lifecycle (detailed comparison)",
    "text": "Excel vs R across the data lifecycle (detailed comparison)\nThis table is clickable. If something is unclear, jump back up.\n\n\n\nData lifecycle stage\nExcel: role & limits\nR: role & strengths\n\n\n\n\nData generation\n‚ùå Not involved. Excel does not generate empirical data; it only records values after they exist.\n‚ùå Not involved for empirical data. R generates data only in simulations or models.\n\n\nData entry & initial structuring\n‚úÖ Very strong. Designed for human input, fast manual entry, immediate visual feedback, simple validation rules, and readable tables. ‚ö†Ô∏è Risk of auto-formatting and silent type conversion.\n‚ö†Ô∏è Not suitable. R is not designed for manual data entry by humans and expects data to already exist in files or databases.\n\n\nSanity checks\n‚úÖ Good. Sorting, filtering, and visual inspection help detect obvious errors and missing values. ‚ö†Ô∏è Checks are informal and not documented.\n‚úÖ Very strong. Programmatic summaries, range checks, and consistency tests that are explicit, repeatable, and auditable.\n\n\nData cleaning & harmonization\n‚ö†Ô∏è Limited and risky. Cleaning is manual, undocumented, and prone to copy-paste errors; changes are hard to trace or reproduce.\n‚úÖ Excellent. Cleaning steps are scripted, transparent, reproducible, and can be reviewed or rerun exactly.\n\n\nData storage & archiving\n‚ùå Poor choice. Excel files mix data, formatting, and logic; they hide metadata, are fragile across versions, and are not diff-friendly. Excel is not a database. However, can save (CSV TSV)\n‚ö†Ô∏è Indirect role. R does not store data long-term but produces clean, portable outputs (CSV, databases) suitable for archiving and sharing.\n\n\nData analysis & statistics\n‚ùå Danger zone. Formulas are opaque, errors are hard to detect, analyses are not reproducible, and reviewers cannot re-run the workflow.\n‚úÖ Core strength. Transparent statistical analysis, modelling, simulations, diagnostics, and hypothesis testing with full reproducibility.\n\n\nVisualization & communication\n‚ö†Ô∏è Limited. Useful for quick exploratory plots and sanity checks, but poor for consistent, reproducible, publication-quality figures.\n‚úÖ Excellent. Scripted, consistent, and publication-quality visualizations with full control over logic and aesthetics.\n\n\nReporting & communication\n‚ö†Ô∏è Manual and fragile. Copy-paste workflows lead to version drift and broken links between data, analysis, and results.\n‚úÖ Excellent (with Quarto). Code, results, figures, and narrative are generated together in one reproducible document.\n\n\nReproducibility & reuse\n‚ùå Actively hostile. No clear record of what was changed, when, or why; workflows are person-specific and irreproducible.\n‚úÖ Designed for this. Scripts and version control document every step, enabling reuse, verification, and peer review.\n\n\n\n\n\n\n\n\n\n## Course wrap-up discussion\n\n\nWe‚Äôll now have an open discussion: - What were your biggest ‚Äúaha‚Äù moments or takeaways from this week? - Is there something you wish we had covered more? (We can at least point you to resources for those topics.) - How do you plan to apply these skills in your work or research? - Any remaining questions or scenarios you want to brainstorm about?\n\n\n(The instructor will facilitate a discussion here, ensuring everyone has a chance to voice their thoughts.)\n\n\nWe also have the official course evaluation to complete ‚Äì your feedback helps improve this course for the future.\n\n\nThank you for engaging wholeheartedly in these five days. You‚Äôve not only learned specific skills (Excel tricks, R coding, etc.) but also a philosophy of data competence that will serve you across projects. Keep that inquisitive and meticulous spirit alive as you go forth ‚Äì whether you‚Äôre managing a conservation program‚Äôs data or delving into research, your digital toolset and mindset will be crucial tools in your toolkit.\n\n\nGood luck, and stay in touch with the community of practice ‚Äì we‚Äôre all continuously learning in this rapidly evolving field.\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nWhat is one concrete step you will take in the next month to further improve your data skills (e.g., complete an online R tutorial, restructure an existing dataset, start using Git for a project)?\nIn what ways has your view of data or data analysis changed over the duration of this course?\n\nFinally, please fill out the course feedback form (your responses are anonymous and greatly appreciated):\n- Anonymous feedback & course evaluation",
    "crumbs": [
      "Course Days",
      "Day 5 ‚Äì Tools, models, and scientific responsibility"
    ]
  },
  {
    "objectID": "references/r-operators.html",
    "href": "references/r-operators.html",
    "title": "R Operators Cheat Sheet",
    "section": "",
    "text": "This document lists the main operator groups in R, with tables and minimal examples.\nOperators are symbols (and a few special names) that act on values or objects, e.g., +, &lt;-, ==, %in%.",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#overview",
    "href": "references/r-operators.html#overview",
    "title": "R Operators Cheat Sheet",
    "section": "",
    "text": "This document lists the main operator groups in R, with tables and minimal examples.\nOperators are symbols (and a few special names) that act on values or objects, e.g., +, &lt;-, ==, %in%.",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#assignment-operators",
    "href": "references/r-operators.html#assignment-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Assignment operators",
    "text": "Assignment operators\n\n\n\nOperator\nMeaning\nNotes\n\n\n\n\n&lt;-\nassign (preferred)\nstandard in R\n\n\n-&gt;\nassign (rightwards)\nrare\n\n\n=\nassign in some contexts\nsafe in function arguments\n\n\n&lt;&lt;-\nassign in parent environment\nadvanced\n\n\n\nx &lt;- 10\n10 -&gt; y",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#arithmetic-operators",
    "href": "references/r-operators.html#arithmetic-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Arithmetic operators",
    "text": "Arithmetic operators\n\n\n\nOperator\nMeaning\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^\nexponentiation\n\n\n%%\nmodulo\n\n\n%/%\ninteger division\n\n\n\n2 + 3\n10 %% 3\n10 %/% 3",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#relational-operators",
    "href": "references/r-operators.html#relational-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Relational operators",
    "text": "Relational operators\n\n\n\nOperator\nMeaning\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal\n\n\n==\nequal\n\n\n!=\nnot equal\n\n\n\nx &lt;- c(3, 5, 7)\nx &gt; 4",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#logical-operators",
    "href": "references/r-operators.html#logical-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Logical operators",
    "text": "Logical operators\n\nElement-wise\n\n\n\nOperator\nMeaning\n\n\n\n\n&\nAND\n\n\n|\nOR\n\n\n!\nNOT\n\n\n\n(x &gt; 4) & (x &lt; 7)\n\n\nShort-circuit\n\n\n\nOperator\nMeaning\n\n\n\n\n&&\nAND\n\n\n||\nOR\n\n\n\nTRUE && FALSE",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#membership-operator",
    "href": "references/r-operators.html#membership-operator",
    "title": "R Operators Cheat Sheet",
    "section": "Membership operator",
    "text": "Membership operator\n\n\n\nOperator\nMeaning\n\n\n\n\n%in%\nelement in set\n\n\n\nc(\"oak\",\"pine\") %in% c(\"oak\",\"beech\",\"pine\")",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#indexing-operators",
    "href": "references/r-operators.html#indexing-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Indexing operators",
    "text": "Indexing operators\n\n\n\nOperator\nMeaning\n\n\n\n\n[\nsubset\n\n\n[[\nextract element\n\n\n$\nextract by name\n\n\n\nlst &lt;- list(a=1:3,b=4)\nlst$a",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#sequence-operator",
    "href": "references/r-operators.html#sequence-operator",
    "title": "R Operators Cheat Sheet",
    "section": "Sequence operator",
    "text": "Sequence operator\n\n\n\nOperator\nMeaning\n\n\n\n\n:\ninteger sequence\n\n\n\n1:5",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#namespace-operators",
    "href": "references/r-operators.html#namespace-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Namespace operators",
    "text": "Namespace operators\n\n\n\nOperator\nMeaning\n\n\n\n\n::\nexported object\n\n\n:::\ninternal object\n\n\n\nstats::median(1:5)",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#formula-operator",
    "href": "references/r-operators.html#formula-operator",
    "title": "R Operators Cheat Sheet",
    "section": "Formula operator",
    "text": "Formula operator\n\n\n\nOperator\nMeaning\n\n\n\n\n~\nformula\n\n\n\nlm(y ~ x, data=df)",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#pipe-operators",
    "href": "references/r-operators.html#pipe-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Pipe operators",
    "text": "Pipe operators\n\n\n\nOperator\nMeaning\n\n\n\n\n|&gt;\nbase pipe\n\n\n%&gt;%\nmagrittr pipe\n\n\n\n1:5 |&gt; mean()",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#special-infix-operators",
    "href": "references/r-operators.html#special-infix-operators",
    "title": "R Operators Cheat Sheet",
    "section": "Special infix operators",
    "text": "Special infix operators\n\n\n\nOperator\nMeaning\n\n\n\n\n%*%\nmatrix multiplication\n\n\n%o%\nouter product\n\n\n%x%\nKronecker product",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  },
  {
    "objectID": "references/r-operators.html#summary",
    "href": "references/r-operators.html#summary",
    "title": "R Operators Cheat Sheet",
    "section": "Summary",
    "text": "Summary\n\n\n\nGroup\nOperators\n\n\n\n\nAssignment\n&lt;-, -&gt;, =, &lt;&lt;-\n\n\nArithmetic\n+, -, *, /, ^, %%, %/%\n\n\nRelational\n&lt;, &lt;=, &gt;, &gt;=, ==, !=\n\n\nLogical\n&, |, !, &&, ||\n\n\nMembership\n%in%\n\n\nIndexing\n[, [[, $\n\n\nPipes\n|&gt;, %&gt;%",
    "crumbs": [
      "Reference",
      "R Operators Cheat Sheet"
    ]
  }
]